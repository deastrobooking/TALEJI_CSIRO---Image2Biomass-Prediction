{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247362b9",
   "metadata": {
    "papermill": {
     "duration": 0.00392,
     "end_time": "2025-11-02T05:01:52.034634",
     "exception": false,
     "start_time": "2025-11-02T05:01:52.030714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CSIRO Image2Biomass Prediction - Enhanced Baseline\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### For Kaggle Users\n",
    "If running on Kaggle, first upload the requirements file to your working directory, then run:\n",
    "```python\n",
    "# Option 1: Minimal requirements (faster install)\n",
    "!pip -q install -r /kaggle/working/requirements-min.txt\n",
    "\n",
    "# Option 2: Full requirements (includes R interop and notebook utilities)  \n",
    "!pip -q install -r /kaggle/working/requirements.txt\n",
    "```\n",
    "\n",
    "### For Local/Colab Users\n",
    "```python\n",
    "# Install from the repository requirements\n",
    "!pip -q install -r requirements-min.txt\n",
    "# or \n",
    "!pip -q install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "âœ… **RGB Image Features**: 60-80 visual features (color, texture, vegetation indices)  \n",
    "âœ… **Log-space Training**: Handles skewed biomass distributions  \n",
    "âœ… **Isotonic Calibration**: Improves prediction reliability  \n",
    "âœ… **Physics Constraints**: Enforces biological relationships  \n",
    "âœ… **Conformal Intervals**: Provides uncertainty quantification  \n",
    "âœ… **Robust Pipeline**: Handles missing data and edge cases  \n",
    "\n",
    "## Expected Performance\n",
    "\n",
    "This enhanced baseline typically achieves:\n",
    "- **Individual RÂ²**: 0.3-0.7+ per target (varies by target complexity)\n",
    "- **Weighted RÂ²**: 0.4-0.6+ (competition metric)\n",
    "- **Key improvements**: ~10-20% boost from RGB features + log-space training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64888b52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T05:01:52.043469Z",
     "iopub.status.busy": "2025-11-02T05:01:52.043026Z",
     "iopub.status.idle": "2025-11-02T05:01:57.883033Z",
     "shell.execute_reply": "2025-11-02T05:01:57.881683Z"
    },
    "papermill": {
     "duration": 5.846625,
     "end_time": "2025-11-02T05:01:57.884826",
     "exception": false,
     "start_time": "2025-11-02T05:01:52.038201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Detected Kaggle environment - using pre-installed packages\n",
      "âœ“ All required packages already available\n",
      "âœ“ Image processing libraries loaded\n",
      "âœ“ rpy2 available for R integration\n",
      "âœ“ Optuna available for hyperparameter optimization\n",
      "âœ“ scikit-optimize available\n",
      "\n",
      "============================================================\n",
      "ðŸš€ ENVIRONMENT SUMMARY\n",
      "============================================================\n",
      "Environment: Kaggle\n",
      "Professional Mode: True\n",
      "RGB Features: âœ“ Enabled\n",
      "R Integration: âœ“ Available\n",
      "Advanced Tuning: âœ“ Available\n",
      "Python Version: 3.11.13\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Kaggle-Optimized Package Detection and Imports\n",
    "# ===============================================================\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Toggle professional-only settings\n",
    "PROFESSIONAL_MODE = True\n",
    "\n",
    "def is_kaggle_environment():\n",
    "    \"\"\"Detect if running in Kaggle environment.\"\"\"\n",
    "    return Path('/kaggle').exists() or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "def install_missing_packages():\n",
    "    \"\"\"Only install packages that are actually missing, with Kaggle-optimized approach.\"\"\"\n",
    "    \n",
    "    # For Kaggle: Most packages are pre-installed, avoid requirements files\n",
    "    if is_kaggle_environment():\n",
    "        print(\"âœ“ Detected Kaggle environment - using pre-installed packages\")\n",
    "        \n",
    "        # Only install if specific packages are missing\n",
    "        missing_packages = []\n",
    "        \n",
    "        # Check critical imaging packages\n",
    "        try:\n",
    "            import cv2\n",
    "        except ImportError:\n",
    "            missing_packages.append(\"opencv-python-headless\")\n",
    "            \n",
    "        try:\n",
    "            import skimage\n",
    "        except ImportError:\n",
    "            missing_packages.append(\"scikit-image\")\n",
    "        \n",
    "        if missing_packages:\n",
    "            print(f\"Installing missing packages: {missing_packages}\")\n",
    "            try:\n",
    "                for pkg in missing_packages:\n",
    "                    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "                print(\"âœ“ Missing packages installed successfully\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not install {missing_packages}: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"âœ“ All required packages already available\")\n",
    "            return True\n",
    "    \n",
    "    else:\n",
    "        # For local environments: Try requirements files\n",
    "        print(\"âœ“ Detected local environment - checking requirements files\")\n",
    "        req_paths = [\n",
    "            Path('./requirements-min.txt'),\n",
    "            Path('./requirements.txt'),\n",
    "        ]\n",
    "        \n",
    "        for req_file in req_paths:\n",
    "            if req_file.exists():\n",
    "                print(f\"Installing from {req_file}\")\n",
    "                try:\n",
    "                    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(req_file)])\n",
    "                    print(\"âœ“ Requirements installed successfully\")\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not install from {req_file}: {e}\")\n",
    "        \n",
    "        # Fallback: install critical packages individually\n",
    "        print(\"Installing critical packages individually...\")\n",
    "        critical_packages = [\n",
    "            \"numpy>=1.26\", \"pandas>=2.1\", \"scikit-learn>=1.3\", \n",
    "            \"scikit-image>=0.22\", \"opencv-python-headless>=4.8.1\", \"pyarrow>=12\"\n",
    "        ]\n",
    "        try:\n",
    "            for pkg in critical_packages:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "            print(\"âœ“ Critical packages installed successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Package installation failed: {e}\")\n",
    "            return False\n",
    "\n",
    "# Install missing packages (Kaggle-optimized)\n",
    "_installed = install_missing_packages()\n",
    "\n",
    "# Import core libraries (should work in both Kaggle and local)\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Import image processing libraries with graceful fallback\n",
    "cv2_ok = False\n",
    "skimage_ok = False\n",
    "try:\n",
    "    import cv2\n",
    "    from skimage import filters, feature\n",
    "    from skimage.util import img_as_ubyte\n",
    "    cv2_ok = True\n",
    "    skimage_ok = True\n",
    "    print('âœ“ Image processing libraries loaded')\n",
    "except ImportError as e:\n",
    "    print(f'âš  Image processing libraries not available: {e}')\n",
    "    print('RGB features will be disabled.')\n",
    "\n",
    "# Import ML libraries (standard in Kaggle)\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Optional packages (graceful degradation)\n",
    "rpy2_ok = False\n",
    "try:\n",
    "    import rpy2\n",
    "    rpy2_ok = True\n",
    "    print('âœ“ rpy2 available for R integration')\n",
    "except ImportError:\n",
    "    print('âš  rpy2 not available - R features disabled')\n",
    "\n",
    "optuna_ok = False\n",
    "try:\n",
    "    import optuna\n",
    "    optuna_ok = True\n",
    "    print('âœ“ Optuna available for hyperparameter optimization')\n",
    "except ImportError:\n",
    "    print('âš  Optuna not available - advanced tuning disabled')\n",
    "\n",
    "skopt_ok = False\n",
    "try:\n",
    "    import skopt\n",
    "    skopt_ok = True\n",
    "    print('âœ“ scikit-optimize available')\n",
    "except ImportError:\n",
    "    print('âš  scikit-optimize not available')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------- Competition Configuration ---------\n",
    "WEIGHTS = {\n",
    "    \"Dry_Green_g\": 0.1,\n",
    "    \"Dry_Dead_g\": 0.1,\n",
    "    \"Dry_Clover_g\": 0.1,\n",
    "    \"GDM_g\": 0.2,\n",
    "    \"Dry_Total_g\": 0.5,\n",
    "}\n",
    "TARGETS = list(WEIGHTS.keys())\n",
    "\n",
    "# --------- Feature Configuration ---------\n",
    "USE_LOG_SPACE = True\n",
    "USE_ISOTONIC_CALIBRATION = True\n",
    "APPLY_PHYSICS_CONSTRAINTS = True\n",
    "ENABLE_RGB_FEATURES = cv2_ok and skimage_ok\n",
    "ENABLE_PROFESSIONAL_FEATURES = PROFESSIONAL_MODE and (optuna_ok or skopt_ok)\n",
    "\n",
    "def r2_manual(y_true, y_pred):\n",
    "    \"\"\"Manual RÂ² calculation matching competition metric.\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    if y_true.size == 0:\n",
    "        return np.nan\n",
    "    y_bar = y_true.mean()\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - y_bar)**2)\n",
    "    if ss_tot == 0:\n",
    "        return 1.0 if np.allclose(y_true, y_pred) else 0.0\n",
    "    return 1.0 - ss_res/ss_tot\n",
    "\n",
    "# Environment Summary\n",
    "print('\\n' + '='*60)\n",
    "print('ðŸš€ ENVIRONMENT SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Environment: {\"Kaggle\" if is_kaggle_environment() else \"Local/Other\"}')\n",
    "print(f'Professional Mode: {PROFESSIONAL_MODE}')\n",
    "print(f'RGB Features: {\"âœ“ Enabled\" if ENABLE_RGB_FEATURES else \"âœ— Disabled\"}')\n",
    "print(f'R Integration: {\"âœ“ Available\" if rpy2_ok else \"âœ— Disabled\"}')\n",
    "print(f'Advanced Tuning: {\"âœ“ Available\" if ENABLE_PROFESSIONAL_FEATURES else \"âœ— Disabled\"}')\n",
    "print(f'Python Version: {sys.version.split()[0]}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d076fb3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T05:01:57.893442Z",
     "iopub.status.busy": "2025-11-02T05:01:57.892976Z",
     "iopub.status.idle": "2025-11-02T05:01:57.951354Z",
     "shell.execute_reply": "2025-11-02T05:01:57.950317Z"
    },
    "papermill": {
     "duration": 0.064333,
     "end_time": "2025-11-02T05:01:57.952837",
     "exception": false,
     "start_time": "2025-11-02T05:01:57.888504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1785, 9) | Test shape: (5, 3)\n",
      "Train columns: ['sample_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'target_name', 'target']\n",
      "Test columns: ['sample_id', 'image_path', 'target_name']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Data: load train/test with robust paths\n",
    "# ===============================================================\n",
    "# Primary (Kaggle):\n",
    "KAGGLE_INPUT = Path('/kaggle/input/csiro-biomass')\n",
    "# Fallback (local/dev):\n",
    "LOCAL_INPUTS = [\n",
    "    Path('/kaggle/input'),  # generic\n",
    "    Path('/mnt/data'),      # this environment\n",
    "    Path('.')               # last resort\n",
    "]\n",
    "\n",
    "def resolve_path(filename):\n",
    "    if KAGGLE_INPUT.exists():\n",
    "        p = KAGGLE_INPUT/filename\n",
    "        if p.exists(): return p\n",
    "    for base in LOCAL_INPUTS:\n",
    "        p = base/filename\n",
    "        if p.exists(): return p\n",
    "    raise FileNotFoundError(f\"Could not locate {filename} in known paths.\")\n",
    "\n",
    "train = pd.read_csv(resolve_path('train.csv'))\n",
    "test  = pd.read_csv(resolve_path('test.csv'))\n",
    "sample_sub = pd.read_csv(resolve_path('sample_submission.csv'))\n",
    "\n",
    "print('Train shape:', train.shape, '| Test shape:', test.shape)\n",
    "print('Train columns:', list(train.columns))\n",
    "print('Test columns:', list(test.columns))\n",
    "\n",
    "# Ensure image_id extraction\n",
    "train['image_id'] = train['sample_id'].str.split('__').str[0]\n",
    "test['image_id']  = test['sample_id'].str.split('__').str[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ebc317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T05:01:57.961687Z",
     "iopub.status.busy": "2025-11-02T05:01:57.961366Z",
     "iopub.status.idle": "2025-11-02T05:14:09.860798Z",
     "shell.execute_reply": "2025-11-02T05:14:09.859182Z"
    },
    "papermill": {
     "duration": 731.910519,
     "end_time": "2025-11-02T05:14:09.867070",
     "exception": false,
     "start_time": "2025-11-02T05:01:57.956551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RGB image features...\n",
      "RGB features built - Train: (357, 89), Test: (1, 89)\n",
      "RGB feature columns: 88\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# RGB Image Features (Compact, Submission-Safe)\n",
    "# ===============================================================\n",
    "\n",
    "# Only build RGB features if imaging libraries are available\n",
    "if ENABLE_RGB_FEATURES:\n",
    "    print('Building RGB image features...')\n",
    "    \n",
    "    def _safe_read(p: Path):\n",
    "        try:\n",
    "            if not p.exists(): return None\n",
    "            im = cv2.imread(str(p), cv2.IMREAD_COLOR)\n",
    "            if im is None: return None\n",
    "            return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _color_stats(img, prefix):\n",
    "        a = img.reshape(-1, img.shape[-1]).astype(np.float32)\n",
    "        if a.max() > 1.5: a = a/255.0\n",
    "        out = {}\n",
    "        for i, name in enumerate([\"c0\",\"c1\",\"c2\"]):\n",
    "            v = a[:, i]\n",
    "            out[f\"{prefix}_{name}_mean\"] = float(np.mean(v))\n",
    "            out[f\"{prefix}_{name}_std\"]  = float(np.std(v))\n",
    "            out[f\"{prefix}_{name}_p10\"]  = float(np.percentile(v, 10))\n",
    "            out[f\"{prefix}_{name}_p50\"]  = float(np.percentile(v, 50))\n",
    "            out[f\"{prefix}_{name}_p90\"]  = float(np.percentile(v, 90))\n",
    "        return out\n",
    "\n",
    "    def _veg_indices_rgb(img):\n",
    "        rgb = img.astype(np.float32)\n",
    "        if rgb.max() > 1.5: rgb = rgb/255.0\n",
    "        R, G, B = [rgb[...,i] for i in range(3)]\n",
    "        eps = 1e-6\n",
    "        ExG  = 2*G - R - B\n",
    "        ExR  = 1.4*R - G\n",
    "        ExGR = ExG - ExR\n",
    "        VARI = (G - R) / (G + R - B + eps)\n",
    "        NDI  = (G - R) / (G + R + eps)\n",
    "        CIVE = 0.441*R - 0.881*G + 0.385*B + 18.78745\n",
    "        feats = {}\n",
    "        for name, arr in [(\"exg\",ExG),(\"exr\",ExR),(\"exgr\",ExGR),(\"vari\",VARI),(\"ndi\",NDI),(\"cive\",CIVE)]:\n",
    "            v = arr.reshape(-1)\n",
    "            feats[f\"{name}_mean\"] = float(np.mean(v))\n",
    "            feats[f\"{name}_std\"]  = float(np.std(v))\n",
    "            feats[f\"{name}_p90\"]  = float(np.percentile(v, 90))\n",
    "        try:\n",
    "            thr = filters.threshold_otsu(ExG)\n",
    "            feats[\"green_cover_frac\"] = float((ExG > thr).mean())\n",
    "        except Exception:\n",
    "            feats[\"green_cover_frac\"] = np.nan\n",
    "        return feats\n",
    "\n",
    "    def _texture_features(gray_u8):\n",
    "        out = {}\n",
    "        try:\n",
    "            lbp = feature.local_binary_pattern(gray_u8, P=8, R=1, method='uniform')\n",
    "            n_bins = int(lbp.max() + 1)\n",
    "            hist, _ = np.histogram(lbp, bins=n_bins, range=(0,n_bins), density=True)\n",
    "            hist = hist if len(hist)>=10 else np.pad(hist, (0,10-len(hist)))\n",
    "            for i in range(10): out[f\"lbp_u_hist_{i}\"] = float(hist[i])\n",
    "        except Exception:\n",
    "            for i in range(10): out[f\"lbp_u_hist_{i}\"] = np.nan\n",
    "        try:\n",
    "            q = (gray_u8.astype(np.float32)/255.0*31).astype(np.uint8)\n",
    "            glcm = feature.graycomatrix(q, [1,2,3], [0,np.pi/4,np.pi/2,3*np.pi/4], 32, symmetric=True, normed=True)\n",
    "            for prop in [\"contrast\",\"dissimilarity\",\"homogeneity\",\"ASM\",\"energy\",\"correlation\"]:\n",
    "                M = feature.graycoprops(glcm, prop)\n",
    "                out[f\"glcm_{prop}_mean\"] = float(M.mean())\n",
    "                out[f\"glcm_{prop}_std\"]  = float(M.std())\n",
    "        except Exception:\n",
    "            for prop in [\"contrast\",\"dissimilarity\",\"homogeneity\",\"ASM\",\"energy\",\"correlation\"]:\n",
    "                out[f\"glcm_{prop}_mean\"] = np.nan; out[f\"glcm_{prop}_std\"] = np.nan\n",
    "        try:\n",
    "            edges = feature.canny(gray_u8.astype(np.float32)/255.0, sigma=1.0)\n",
    "            out[\"edge_density\"] = float(edges.mean())\n",
    "        except Exception:\n",
    "            out[\"edge_density\"] = np.nan\n",
    "        return out\n",
    "\n",
    "    def build_rgb_features(df_long: pd.DataFrame, base: Path) -> pd.DataFrame:\n",
    "        one = df_long.sort_values(\"sample_id\").drop_duplicates(\"image_id\")[[\"image_id\",\"image_path\"]].copy()\n",
    "        rows = []\n",
    "        for _, r in one.iterrows():\n",
    "            iid = str(r[\"image_id\"]); rel = str(r.get(\"image_path\",\"\"))\n",
    "            paths = ([base/rel] if rel else []) + [base/\"train\"/f\"{iid}.jpg\", base/\"test\"/f\"{iid}.jpg\"]\n",
    "            img = None\n",
    "            for p in paths:\n",
    "                img = _safe_read(p)\n",
    "                if img is not None: break\n",
    "            feats = {\"image_id\": iid}\n",
    "            if img is None:\n",
    "                feats[\"img_missing\"] = 1.0\n",
    "                rows.append(pd.DataFrame([feats])); continue\n",
    "            feats[\"img_missing\"] = 0.0\n",
    "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "            lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY); gray_u8 = img_as_ubyte(gray)\n",
    "            feats |= _color_stats(img, \"rgb\")\n",
    "            feats |= _color_stats(hsv, \"hsv\")\n",
    "            feats |= _color_stats(lab, \"lab\")\n",
    "            feats |= _veg_indices_rgb(img)\n",
    "            feats |= _texture_features(gray_u8)\n",
    "            rows.append(pd.DataFrame([feats]))\n",
    "        return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # Build RGB features\n",
    "    DATA_ROOT = Path(\"/kaggle/input/csiro-biomass\") if Path(\"/kaggle/input/csiro-biomass\").exists() else Path(\"/mnt/data\")\n",
    "    rgb_train = build_rgb_features(train, DATA_ROOT)\n",
    "    rgb_test  = build_rgb_features(test,  DATA_ROOT)\n",
    "\n",
    "    print(f'RGB features built - Train: {rgb_train.shape}, Test: {rgb_test.shape}')\n",
    "    print(f'RGB feature columns: {len([c for c in rgb_train.columns if c != \"image_id\"])}')\n",
    "else:\n",
    "    print('RGB features disabled - image processing libraries not available')\n",
    "    # Create empty RGB feature dataframes\n",
    "    rgb_train = pd.DataFrame({'image_id': train['image_id'].unique()})\n",
    "    rgb_test = pd.DataFrame({'image_id': test['image_id'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f9d562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T05:14:09.879108Z",
     "iopub.status.busy": "2025-11-02T05:14:09.877957Z",
     "iopub.status.idle": "2025-11-02T05:14:09.931584Z",
     "shell.execute_reply": "2025-11-02T05:14:09.930442Z"
    },
    "papermill": {
     "duration": 0.061486,
     "end_time": "2025-11-02T05:14:09.933746",
     "exception": false,
     "start_time": "2025-11-02T05:14:09.872260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta columns found in test: ['image_id']\n",
      "Have full test features? False\n",
      "Train with features: (357, 99)\n",
      "Test with features: (1, 89)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Feature assembly utilities\n",
    "# ===============================================================\n",
    "\n",
    "META_COLS = ['Sampling_Date','State','Species','Pre_GSHH_NDVI','Height_Ave_cm']\n",
    "\n",
    "# Build one unique row per image_id with meta\n",
    "def extract_meta(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    first_rows = (df_long\n",
    "                  .sort_values('sample_id')\n",
    "                  .drop_duplicates('image_id'))\n",
    "    meta = first_rows[['image_id'] + [c for c in META_COLS if c in first_rows.columns]].copy()\n",
    "    return meta\n",
    "\n",
    "# Pivot targets to wide: one row per image_id, cols = targets\n",
    "def pivot_targets_wide(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    wide = df_long.pivot_table(index='image_id', columns='target_name', values='target', aggfunc='mean')\n",
    "    wide = wide.reindex(columns=TARGETS)  # ensure correct order\n",
    "    wide = wide.reset_index()\n",
    "    return wide\n",
    "\n",
    "train_meta = extract_meta(train)\n",
    "train_wide_targets = pivot_targets_wide(train)\n",
    "\n",
    "# Merge targets with meta\n",
    "train_wide = train_meta.merge(train_wide_targets, on='image_id', how='left')\n",
    "\n",
    "# Merge RGB features into training data\n",
    "train_wide_features = train_wide.merge(rgb_train, on='image_id', how='left')\n",
    "\n",
    "# For test, meta may or may not exist; extract what we can\n",
    "test_meta = extract_meta(test)\n",
    "print('Meta columns found in test:', list(test_meta.columns))\n",
    "\n",
    "# Merge RGB features into test data\n",
    "test_features_df = test_meta.merge(rgb_test, on='image_id', how='left')\n",
    "\n",
    "HAVE_TEST_FEATURES = all(col in test_meta.columns for col in META_COLS)\n",
    "print('Have full test features?', HAVE_TEST_FEATURES)\n",
    "print(f'Train with features: {train_wide_features.shape}')\n",
    "print(f'Test with features: {test_features_df.shape}')\n",
    "\n",
    "# Add helper functions\n",
    "def weighted_r2_from_long(true_long: pd.DataFrame, pred_long: pd.DataFrame):\n",
    "    \"\"\"Calculate weighted RÂ² from long-format dataframes.\"\"\"\n",
    "    merged = (true_long[['sample_id','target_name','target']].rename(columns={'target':'y_true'})\n",
    "              .merge(pred_long[['sample_id','target']].rename(columns={'target':'y_pred'}),\n",
    "                     on='sample_id', how='inner', validate='one_to_one'))\n",
    "    out = {}\n",
    "    final = 0.0\n",
    "    for t in TARGETS:\n",
    "        sub = merged[merged['target_name'] == t]\n",
    "        r2 = r2_manual(sub['y_true'].values, sub['y_pred'].values)\n",
    "        out[t] = float(r2)\n",
    "        final += WEIGHTS[t]*r2\n",
    "    out['final'] = float(final)\n",
    "    return out\n",
    "\n",
    "def preds_wide_to_long(image_ids, preds_wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert wide predictions to long format for submission.\"\"\"\n",
    "    img_ids = list(image_ids)\n",
    "    assert preds_wide.shape[0] == len(img_ids), \"Row count mismatch to image_ids\"\n",
    "    df = preds_wide.copy()\n",
    "    df['image_id'] = img_ids\n",
    "    rows = []\n",
    "    for t in TARGETS:\n",
    "        part = df[['image_id', t]].rename(columns={t:'target'})\n",
    "        part['sample_id'] = part['image_id'] + '__' + t\n",
    "        rows.append(part[['sample_id','target']])\n",
    "    return (pd.concat(rows, ignore_index=True)\n",
    "              .sort_values('sample_id').reset_index(drop=True))\n",
    "\n",
    "def long_submission(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Format long predictions as competition submission.\"\"\"\n",
    "    return df_long[['sample_id','target']].sort_values('sample_id').reset_index(drop=True)\n",
    "\n",
    "def apply_physical_constraints(preds: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply physical constraints to predictions:\n",
    "    1. All values >= 0\n",
    "    2. GDM â‰ˆ Dry_Green + Dry_Clover (soft enforcement via averaging)\n",
    "    3. Dry_Total >= GDM\n",
    "    \"\"\"\n",
    "    preds = preds.copy()\n",
    "    \n",
    "    # Ensure non-negative\n",
    "    for t in TARGETS:\n",
    "        preds[t] = np.maximum(preds[t], 0)\n",
    "    \n",
    "    # Enforce GDM â‰ˆ Dry_Green + Dry_Clover\n",
    "    gdm_from_components = preds['Dry_Green_g'] + preds['Dry_Clover_g']\n",
    "    preds['GDM_g'] = 0.7 * preds['GDM_g'] + 0.3 * gdm_from_components\n",
    "    \n",
    "    # Ensure Dry_Total >= GDM\n",
    "    preds['Dry_Total_g'] = np.maximum(preds['Dry_Total_g'], preds['GDM_g'])\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a16e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T05:14:09.943637Z",
     "iopub.status.busy": "2025-11-02T05:14:09.943169Z",
     "iopub.status.idle": "2025-11-02T05:14:10.838479Z",
     "shell.execute_reply": "2025-11-02T05:14:10.837437Z"
    },
    "papermill": {
     "duration": 0.902188,
     "end_time": "2025-11-02T05:14:10.840162",
     "exception": false,
     "start_time": "2025-11-02T05:14:09.937974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training enhanced Python Ridge models with log-space and RGB features...\n",
      "Training with 93 numeric and 2 categorical features\n",
      "Training Dry_Green_g... OOF RÂ² = 0.7830\n",
      "Training Dry_Dead_g... OOF RÂ² = 0.4629\n",
      "Training Dry_Clover_g... OOF RÂ² = 0.7350\n",
      "Training GDM_g... OOF RÂ² = 0.8022\n",
      "Training Dry_Total_g... OOF RÂ² = 0.7571\n",
      "\n",
      "Enhanced Python baseline weighted RÂ² (OOF):\n",
      "{\n",
      "  \"Dry_Green_g\": 0.7830084281916525,\n",
      "  \"Dry_Dead_g\": 0.4629183127654847,\n",
      "  \"Dry_Clover_g\": 0.7349675566487964,\n",
      "  \"GDM_g\": 0.814580194961435,\n",
      "  \"Dry_Total_g\": 0.7599753279903095,\n",
      "  \"final\": 0.7409931327480352\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Enhanced Python baseline: Ridge with log-space training\n",
    "# ===============================================================\n",
    "\n",
    "def add_date_derivatives(df):\n",
    "    \"\"\"Add date features if Sampling_Date exists.\"\"\"\n",
    "    df = df.copy()\n",
    "    if 'Sampling_Date' in df.columns:\n",
    "        df['Sampling_Date'] = pd.to_datetime(df['Sampling_Date'], errors='coerce')\n",
    "        df['Year'] = df['Sampling_Date'].dt.year\n",
    "        df['Month'] = df['Sampling_Date'].dt.month\n",
    "        df['DayOfYear'] = df['Sampling_Date'].dt.dayofyear\n",
    "    return df\n",
    "\n",
    "def build_feature_lists(df):\n",
    "    \"\"\"Build lists of numeric and categorical features.\"\"\"\n",
    "    # Get base meta columns that exist\n",
    "    base_cols = [c for c in META_COLS if c in df.columns and c != 'Sampling_Date']\n",
    "    \n",
    "    # Add date derivatives if they exist\n",
    "    date_cols = [c for c in ['Year', 'Month', 'DayOfYear'] if c in df.columns]\n",
    "    \n",
    "    # Add RGB features\n",
    "    rgb_cols = [c for c in df.columns if c.startswith(('rgb_', 'hsv_', 'lab_', 'exg', 'exr', 'vari', 'ndi', 'cive', 'green_cover', 'lbp_', 'glcm_', 'edge_', 'img_missing'))]\n",
    "    \n",
    "    # Separate numeric and categorical\n",
    "    all_feature_cols = base_cols + date_cols + rgb_cols\n",
    "    cat_cols = [c for c in all_feature_cols if c in df.columns and df[c].dtype == 'object']\n",
    "    num_cols = [c for c in all_feature_cols if c in df.columns and c not in cat_cols]\n",
    "    \n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def align_columns(df, expected_cols):\n",
    "    \"\"\"Ensure DataFrame has all expected columns.\"\"\"\n",
    "    for col in expected_cols:\n",
    "        if col not in df.columns:\n",
    "            if df[col].dtype == 'object' if col in df.columns else False:\n",
    "                df[col] = 'missing'\n",
    "            else:\n",
    "                df[col] = 0.0\n",
    "    return df[expected_cols + [c for c in df.columns if c not in expected_cols]]\n",
    "\n",
    "def make_preprocess(num_cols, cat_cols):\n",
    "    \"\"\"Create preprocessing pipeline.\"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    transformers = []\n",
    "    \n",
    "    if num_cols:\n",
    "        transformers.append(('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), num_cols))\n",
    "    \n",
    "    if cat_cols:\n",
    "        transformers.append(('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), cat_cols))\n",
    "    \n",
    "    return ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "def fit_predict_ridge_oof_log(train_wide, n_splits=5):\n",
    "    \"\"\"\n",
    "    Fit Ridge models with log-space training and isotonic calibration.\n",
    "    Returns OOF predictions and trained models.\n",
    "    \"\"\"\n",
    "    oof = pd.DataFrame({\"image_id\": train_wide[\"image_id\"].values})\n",
    "    models = {}\n",
    "    \n",
    "    # Prepare features\n",
    "    TW = add_date_derivatives(train_wide)\n",
    "    num_cols, cat_cols = build_feature_lists(TW)\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    groups = TW[\"image_id\"]\n",
    "    \n",
    "    print(f'Training with {len(num_cols)} numeric and {len(cat_cols)} categorical features')\n",
    "    \n",
    "    for t in TARGETS:\n",
    "        print(f'Training {t}...', end=' ')\n",
    "        y_raw = TW[t].values\n",
    "        y = np.log1p(np.clip(y_raw, 0, None))   # log-space\n",
    "        oof_pred_log = np.zeros(len(TW), dtype=float)\n",
    "        oof_pred_raw = np.zeros(len(TW), dtype=float)\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(gkf.split(TW, y, groups=groups)):\n",
    "            X_tr = align_columns(TW.iloc[tr_idx].copy(), list(set(num_cols+cat_cols)))\n",
    "            X_va = align_columns(TW.iloc[va_idx].copy(), list(set(num_cols+cat_cols)))\n",
    "            \n",
    "            pre = make_preprocess(num_cols, cat_cols)\n",
    "            pipe = Pipeline([(\"prep\", pre), (\"ridge\", Ridge(alpha=1.0, random_state=42))])\n",
    "            pipe.fit(X_tr, y[tr_idx])\n",
    "            pred_va_log = pipe.predict(X_va)\n",
    "            pred_va_raw = np.expm1(pred_va_log).clip(min=0)   # back to original units\n",
    "            \n",
    "            oof_pred_log[va_idx] = pred_va_log\n",
    "            oof_pred_raw[va_idx] = pred_va_raw\n",
    "\n",
    "        # Apply isotonic calibration if enabled\n",
    "        if USE_ISOTONIC_CALIBRATION:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip')\n",
    "            iso.fit(oof_pred_raw, y_raw)\n",
    "            oof_pred_final = iso.predict(oof_pred_raw)\n",
    "            calibrator = iso\n",
    "        else:\n",
    "            oof_pred_final = oof_pred_raw\n",
    "            calibrator = None\n",
    "\n",
    "        oof[t] = oof_pred_final\n",
    "\n",
    "        # Fit final model on all data\n",
    "        X_all = align_columns(TW.copy(), list(set(num_cols+cat_cols)))\n",
    "        pre = make_preprocess(num_cols, cat_cols)\n",
    "        final_model = Pipeline([(\"prep\", pre), (\"ridge\", Ridge(alpha=1.0, random_state=42))])\n",
    "        final_model.fit(X_all, y)\n",
    "        models[t] = (final_model, list(set(num_cols+cat_cols)), calibrator)\n",
    "        \n",
    "        r2 = r2_manual(y_raw, oof_pred_final)\n",
    "        print(f'OOF RÂ² = {r2:.4f}')\n",
    "    \n",
    "    return oof, models\n",
    "\n",
    "def predict_with_models_log(models, df_any):\n",
    "    \"\"\"Make predictions using log-space trained models.\"\"\"\n",
    "    DF = add_date_derivatives(df_any.copy())\n",
    "    out = pd.DataFrame({\"image_id\": DF[\"image_id\"].values})\n",
    "    \n",
    "    for t in TARGETS:\n",
    "        model, feats, calibrator = models[t]\n",
    "        X = align_columns(DF.copy(), feats)\n",
    "        pred_log = model.predict(X)\n",
    "        pred_raw = np.expm1(pred_log).clip(min=0)\n",
    "        \n",
    "        # Apply calibration if available\n",
    "        if calibrator is not None:\n",
    "            pred_final = calibrator.predict(pred_raw)\n",
    "        else:\n",
    "            pred_final = pred_raw\n",
    "            \n",
    "        out[t] = pred_final\n",
    "    return out\n",
    "\n",
    "print('Training enhanced Python Ridge models with log-space and RGB features...')\n",
    "python_oof, python_models = fit_predict_ridge_oof_log(train_wide_features)\n",
    "\n",
    "# Apply physics constraints to OOF predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    python_oof_constrained = apply_physical_constraints(python_oof[TARGETS])\n",
    "    python_oof[TARGETS] = python_oof_constrained\n",
    "\n",
    "# Evaluate Python-only OOF using long metric\n",
    "python_oof_long = preds_wide_to_long(train_wide_features['image_id'], python_oof[TARGETS])\n",
    "true_long = train[['sample_id','target_name','target']].copy()\n",
    "py_scores = weighted_r2_from_long(true_long, python_oof_long)\n",
    "print('\\nEnhanced Python baseline weighted RÂ² (OOF):')\n",
    "print(json.dumps(py_scores, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95335a14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T05:14:10.849998Z",
     "iopub.status.busy": "2025-11-02T05:14:10.849679Z",
     "iopub.status.idle": "2025-11-02T05:14:10.950065Z",
     "shell.execute_reply": "2025-11-02T05:14:10.948798Z"
    },
    "papermill": {
     "duration": 0.10736,
     "end_time": "2025-11-02T05:14:10.951860",
     "exception": false,
     "start_time": "2025-11-02T05:14:10.844500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test predictions...\n",
      "Test predictions shape: (1, 6)\n",
      "Test predictions with intervals: (5, 4)\n",
      "Sample predictions with intervals:\n",
      "                    sample_id    target  lower      upper\n",
      "0  ID1001187975__Dry_Clover_g  0.470217    0.0  10.074440\n",
      "1    ID1001187975__Dry_Dead_g  6.082970    0.0  19.296510\n",
      "2   ID1001187975__Dry_Green_g  9.576029    0.0  26.588031\n",
      "3   ID1001187975__Dry_Total_g  8.107964    0.0  28.444929\n",
      "4         ID1001187975__GDM_g  6.142874    0.0  22.776515\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Optional: Conformal Prediction Intervals\n",
    "# ===============================================================\n",
    "\n",
    "def conformal_bounds(oof_true_long, oof_pred_long, test_pred_long, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Add conformal prediction intervals using OOF residuals.\n",
    "    alpha=0.1 -> 90% marginal coverage\n",
    "    \"\"\"\n",
    "    out = test_pred_long.copy()\n",
    "    out['lower'] = out['target']  # Initialize\n",
    "    out['upper'] = out['target']  # Initialize\n",
    "    \n",
    "    for t in TARGETS:\n",
    "        # Get OOF residuals for this target\n",
    "        tmask = oof_true_long[\"target_name\"] == t\n",
    "        if tmask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        residuals = (oof_true_long.loc[tmask, \"target\"].values\n",
    "                    - oof_pred_long.loc[tmask, \"target\"].values)\n",
    "        \n",
    "        # Compute conformal quantile\n",
    "        q = np.quantile(np.abs(residuals), 1 - alpha)\n",
    "        \n",
    "        # Apply to test predictions for this target\n",
    "        tmask2 = out[\"sample_id\"].str.endswith(\"__\" + t)\n",
    "        out.loc[tmask2, \"lower\"] = np.maximum(0, out.loc[tmask2, \"target\"] - q)\n",
    "        out.loc[tmask2, \"upper\"] = out.loc[tmask2, \"target\"] + q\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Generate test predictions\n",
    "print('Generating test predictions...')\n",
    "python_test_preds = predict_with_models_log(python_models, test_features_df)\n",
    "\n",
    "# Apply physics constraints to test predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    python_test_preds_constrained = apply_physical_constraints(python_test_preds[TARGETS])\n",
    "    python_test_preds[TARGETS] = python_test_preds_constrained\n",
    "\n",
    "# Convert to long format\n",
    "python_test_long = preds_wide_to_long(test_features_df['image_id'], python_test_preds[TARGETS])\n",
    "\n",
    "# Add conformal intervals (90% coverage)\n",
    "python_test_with_intervals = conformal_bounds(true_long, python_oof_long, python_test_long, alpha=0.1)\n",
    "\n",
    "print(f'Test predictions shape: {python_test_preds.shape}')\n",
    "print(f'Test predictions with intervals: {python_test_with_intervals.shape}')\n",
    "print('Sample predictions with intervals:')\n",
    "print(python_test_with_intervals.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee998e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T05:14:10.961724Z",
     "iopub.status.busy": "2025-11-02T05:14:10.961367Z",
     "iopub.status.idle": "2025-11-02T05:14:13.048581Z",
     "shell.execute_reply": "2025-11-02T05:14:13.047504Z"
    },
    "papermill": {
     "duration": 2.09441,
     "end_time": "2025-11-02T05:14:13.050673",
     "exception": false,
     "start_time": "2025-11-02T05:14:10.956263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpy2.ipython extension loaded.\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# R modeling via rpy2 (if available)\n",
    "# ===============================================================\n",
    "have_r = False\n",
    "if rpy2_ok:\n",
    "    try:\n",
    "        get_ipython().run_line_magic('load_ext', 'rpy2.ipython')\n",
    "        have_r = True\n",
    "        print('rpy2.ipython extension loaded.')\n",
    "    except Exception as e:\n",
    "        print('Could not load rpy2.ipython. R part will be skipped unless available.\\n', e)\n",
    "else:\n",
    "    print('rpy2 not available - R modeling disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de9fab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T05:14:13.061287Z",
     "iopub.status.busy": "2025-11-02T05:14:13.060303Z",
     "iopub.status.idle": "2025-11-02T05:14:13.086352Z",
     "shell.execute_reply": "2025-11-02T05:14:13.084539Z"
    },
    "papermill": {
     "duration": 0.033752,
     "end_time": "2025-11-02T05:14:13.088604",
     "exception": false,
     "start_time": "2025-11-02T05:14:13.054852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission created!\n",
      "Submission shape: (5, 2)\n",
      "Submission head:\n",
      "                    sample_id    target\n",
      "0  ID1001187975__Dry_Clover_g  0.470217\n",
      "1    ID1001187975__Dry_Dead_g  6.082970\n",
      "2   ID1001187975__Dry_Green_g  9.576029\n",
      "3   ID1001187975__Dry_Total_g  8.107964\n",
      "4         ID1001187975__GDM_g  6.142874\n",
      "\n",
      "Submission statistics by target:\n",
      "Dry_Green_g    : mean=   9.576, std=     nan, min=   9.576, max=   9.576\n",
      "Dry_Dead_g     : mean=   6.083, std=     nan, min=   6.083, max=   6.083\n",
      "Dry_Clover_g   : mean=   0.470, std=     nan, min=   0.470, max=   0.470\n",
      "GDM_g          : mean=   6.143, std=     nan, min=   6.143, max=   6.143\n",
      "Dry_Total_g    : mean=   8.108, std=     nan, min=   8.108, max=   8.108\n",
      "\n",
      "Validation: Expected 5 samples, got 5\n",
      "All targets covered: True\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Final submission generation\n",
    "# ===============================================================\n",
    "\n",
    "# Create final submission using enhanced Python predictions\n",
    "final_submission = long_submission(python_test_long)\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Submission created!')\n",
    "print(f'Submission shape: {final_submission.shape}')\n",
    "print('Submission head:')\n",
    "print(final_submission.head(10))\n",
    "\n",
    "print('\\nSubmission statistics by target:')\n",
    "for target in TARGETS:\n",
    "    target_mask = final_submission['sample_id'].str.endswith(f'__{target}')\n",
    "    target_preds = final_submission.loc[target_mask, 'target']\n",
    "    print(f'{target:15s}: mean={target_preds.mean():8.3f}, std={target_preds.std():8.3f}, '\n",
    "          f'min={target_preds.min():8.3f}, max={target_preds.max():8.3f}')\n",
    "\n",
    "# Validation check\n",
    "expected_samples = len(test['image_id'].unique()) * len(TARGETS)\n",
    "actual_samples = len(final_submission)\n",
    "print(f'\\nValidation: Expected {expected_samples} samples, got {actual_samples}')\n",
    "print(f'All targets covered: {set(final_submission.sample_id.str.split(\"__\").str[1]) == set(TARGETS)}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 747.326251,
   "end_time": "2025-11-02T05:14:14.015232",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-02T05:01:46.688981",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
