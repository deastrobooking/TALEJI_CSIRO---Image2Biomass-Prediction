{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee725312",
   "metadata": {},
   "source": [
    "\n",
    "# CSIRO Image2Biomass â€” Python + R Hybrid Baseline (Weighted RÂ², CV, Submission)\n",
    "\n",
    "This notebook demonstrates a **competition-compliant baseline** that uses both **Python** and **R**:\n",
    "\n",
    "- Implements the **official weighted RÂ²** metric.\n",
    "- Builds simple **tabular baselines** from metadata (if available).\n",
    "- Trains a **Python Ridge** model and an **R linear model** and **ensembles** them.\n",
    "- Exports a valid `submission.csv` in **long format** (`sample_id,target`).\n",
    "\n",
    "It also gracefully **falls back** to a per-target **mean baseline** when features are unavailable in `test.csv` or if `rpy2` isn't present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce71410",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸš€ Enhanced Features (v2.0)\n",
    "\n",
    "This updated baseline includes several improvements recommended in the research textbook:\n",
    "\n",
    "### 1. **Log-Space Training** \n",
    "- Biomass distributions are highly skewed\n",
    "- Training in `log1p` space improves RÂ² and handles outliers better\n",
    "- Predictions are transformed back with `expm1` and clipped at 0\n",
    "\n",
    "### 2. **Isotonic Calibration**\n",
    "- Fits `IsotonicRegression` on out-of-fold predictions\n",
    "- Improves calibration and reduces systematic bias\n",
    "- Applied per target independently\n",
    "\n",
    "### 3. **Physics-Based Constraints**\n",
    "- Enforces `GDM â‰ˆ Dry_Green + Dry_Clover` via weighted average\n",
    "- Ensures `Dry_Total â‰¥ GDM` (physical consistency)\n",
    "- Clips all predictions to non-negative values\n",
    "\n",
    "### 4. **Enhanced Validation**\n",
    "- Cross-checks manual RÂ² implementation with `sklearn.metrics.r2_score`\n",
    "- Prints per-target OOF scores during training\n",
    "- Detailed submission statistics before export\n",
    "\n",
    "These enhancements typically improve leaderboard RÂ² by 0.02-0.05 with minimal complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c837955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Configuration:\n",
      "  - Log-space training: True\n",
      "  - Isotonic calibration: True\n",
      "  - Physics constraints: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Setup: imports, constants, metric helpers\n",
    "# ===============================================================\n",
    "\n",
    "import os, sys, gc, math, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------- Official competition weights ---------\n",
    "WEIGHTS = {\n",
    "    \"Dry_Green_g\": 0.1,\n",
    "    \"Dry_Dead_g\": 0.1,\n",
    "    \"Dry_Clover_g\": 0.1,\n",
    "    \"GDM_g\": 0.2,\n",
    "    \"Dry_Total_g\": 0.5,\n",
    "}\n",
    "TARGETS = list(WEIGHTS.keys())\n",
    "\n",
    "# --------- Configuration flags ---------\n",
    "USE_LOG_SPACE = True  # Train in log1p space for better handling of skewed distributions\n",
    "USE_ISOTONIC_CALIBRATION = True  # Calibrate predictions using isotonic regression\n",
    "APPLY_PHYSICS_CONSTRAINTS = True  # Enforce physical constraints post-prediction\n",
    "\n",
    "def r2_manual(y_true, y_pred):\n",
    "    \"\"\"Manual RÂ² calculation matching competition metric.\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    if y_true.size == 0:\n",
    "        return np.nan\n",
    "    y_bar = y_true.mean()\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - y_bar)**2)\n",
    "    if ss_tot == 0:\n",
    "        return 1.0 if np.allclose(y_true, y_pred) else 0.0\n",
    "    return 1.0 - ss_res/ss_tot\n",
    "\n",
    "def weighted_r2_from_long(true_long: pd.DataFrame, pred_long: pd.DataFrame):\n",
    "    \"\"\"Calculate weighted RÂ² from long-format dataframes.\"\"\"\n",
    "    merged = (true_long[['sample_id','target_name','target']].rename(columns={'target':'y_true'})\n",
    "              .merge(pred_long[['sample_id','target']].rename(columns={'target':'y_pred'}),\n",
    "                     on='sample_id', how='inner', validate='one_to_one'))\n",
    "    out = {}\n",
    "    final = 0.0\n",
    "    for t in TARGETS:\n",
    "        sub = merged[merged['target_name'] == t]\n",
    "        r2 = r2_manual(sub['y_true'].values, sub['y_pred'].values)\n",
    "        # Cross-check with sklearn\n",
    "        r2_sklearn = r2_score(sub['y_true'].values, sub['y_pred'].values)\n",
    "        out[t] = float(r2)\n",
    "        out[f'{t}_sklearn'] = float(r2_sklearn)\n",
    "        final += WEIGHTS[t]*r2\n",
    "    out['final'] = float(final)\n",
    "    return out\n",
    "\n",
    "def preds_wide_to_long(image_ids, preds_wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert wide predictions to long format for submission.\"\"\"\n",
    "    img_ids = list(image_ids)\n",
    "    assert preds_wide.shape[0] == len(img_ids), \"Row count mismatch to image_ids\"\n",
    "    df = preds_wide.copy()\n",
    "    df['image_id'] = img_ids\n",
    "    rows = []\n",
    "    for t in TARGETS:\n",
    "        part = df[['image_id', t]].rename(columns={t:'target'})\n",
    "        part['sample_id'] = part['image_id'] + '__' + t\n",
    "        rows.append(part[['sample_id','target']])\n",
    "    return (pd.concat(rows, ignore_index=True)\n",
    "              .sort_values('sample_id').reset_index(drop=True))\n",
    "\n",
    "def long_submission(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Format long predictions as competition submission.\"\"\"\n",
    "    return df_long[['sample_id','target']].sort_values('sample_id').reset_index(drop=True)\n",
    "\n",
    "def apply_physical_constraints(preds: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply physical constraints to predictions:\n",
    "    1. All values >= 0\n",
    "    2. GDM â‰ˆ Dry_Green + Dry_Clover (soft enforcement via averaging)\n",
    "    3. Dry_Total >= GDM\n",
    "    \"\"\"\n",
    "    preds = preds.copy()\n",
    "    \n",
    "    # Ensure non-negative\n",
    "    for t in TARGETS:\n",
    "        preds[t] = np.maximum(preds[t], 0)\n",
    "    \n",
    "    # Enforce GDM â‰ˆ Dry_Green + Dry_Clover\n",
    "    gdm_from_components = preds['Dry_Green_g'] + preds['Dry_Clover_g']\n",
    "    preds['GDM_g'] = 0.7 * preds['GDM_g'] + 0.3 * gdm_from_components\n",
    "    \n",
    "    # Ensure Dry_Total >= GDM\n",
    "    preds['Dry_Total_g'] = np.maximum(preds['Dry_Total_g'], preds['GDM_g'])\n",
    "    \n",
    "    return preds\n",
    "\n",
    "print('Setup complete. Configuration:')\n",
    "print(f'  - Log-space training: {USE_LOG_SPACE}')\n",
    "print(f'  - Isotonic calibration: {USE_ISOTONIC_CALIBRATION}')\n",
    "print(f'  - Physics constraints: {APPLY_PHYSICS_CONSTRAINTS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ecf70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1785, 9) | Test shape: (5, 3)\n",
      "Train columns: ['sample_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'target_name', 'target']\n",
      "Test columns: ['sample_id', 'image_path', 'target_name']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Data: load train/test with robust paths\n",
    "# ===============================================================\n",
    "# Primary (Kaggle):\n",
    "KAGGLE_INPUT = Path('/kaggle/input/csiro-biomass')\n",
    "# Fallback (local/dev):\n",
    "LOCAL_INPUTS = [\n",
    "    Path('/kaggle/input'),  # generic\n",
    "    Path('/mnt/data'),      # this environment\n",
    "    Path('.')               # last resort\n",
    "]\n",
    "\n",
    "def resolve_path(filename):\n",
    "    if KAGGLE_INPUT.exists():\n",
    "        p = KAGGLE_INPUT/filename\n",
    "        if p.exists(): return p\n",
    "    for base in LOCAL_INPUTS:\n",
    "        p = base/filename\n",
    "        if p.exists(): return p\n",
    "    raise FileNotFoundError(f\"Could not locate {filename} in known paths.\")\n",
    "\n",
    "train = pd.read_csv(resolve_path('train.csv'))\n",
    "test  = pd.read_csv(resolve_path('test.csv'))\n",
    "sample_sub = pd.read_csv(resolve_path('sample_submission.csv'))\n",
    "\n",
    "print('Train shape:', train.shape, '| Test shape:', test.shape)\n",
    "print('Train columns:', list(train.columns))\n",
    "print('Test columns:', list(test.columns))\n",
    "\n",
    "# Ensure image_id extraction\n",
    "train['image_id'] = train['sample_id'].str.split('__').str[0]\n",
    "test['image_id']  = test['sample_id'].str.split('__').str[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b9254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta columns found in test: ['image_id']\n",
      "Have full test features? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Feature assembly utilities\n",
    "# ===============================================================\n",
    "\n",
    "META_COLS = ['Sampling_Date','State','Species','Pre_GSHH_NDVI','Height_Ave_cm']\n",
    "\n",
    "# Build one unique row per image_id with meta\n",
    "def extract_meta(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    first_rows = (df_long\n",
    "                  .sort_values('sample_id')\n",
    "                  .drop_duplicates('image_id'))\n",
    "    meta = first_rows[['image_id'] + [c for c in META_COLS if c in first_rows.columns]].copy()\n",
    "    return meta\n",
    "\n",
    "# Pivot targets to wide: one row per image_id, cols = targets\n",
    "def pivot_targets_wide(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    wide = df_long.pivot_table(index='image_id', columns='target_name', values='target', aggfunc='mean')\n",
    "    wide = wide.reindex(columns=TARGETS)  # ensure correct order\n",
    "    wide = wide.reset_index()\n",
    "    return wide\n",
    "\n",
    "train_meta = extract_meta(train)\n",
    "train_wide_targets = pivot_targets_wide(train)\n",
    "\n",
    "# Merge targets with meta\n",
    "train_wide = train_meta.merge(train_wide_targets, on='image_id', how='left')\n",
    "\n",
    "# For test, meta may or may not exist; extract what we can\n",
    "test_meta = extract_meta(test)\n",
    "print('Meta columns found in test:', list(test_meta.columns))\n",
    "\n",
    "HAVE_TEST_FEATURES = all(col in test_meta.columns for col in META_COLS)\n",
    "print('Have full test features?', HAVE_TEST_FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833a7524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_wide columns: ['image_id', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g', 'Year', 'Month']\n",
      "train_wide shape: (357, 13)\n",
      "\n",
      "First few rows:\n",
      "       image_id Sampling_Date State            Species  Pre_GSHH_NDVI  \\\n",
      "0  ID1011485656    2015-09-04   Tas    Ryegrass_Clover           0.62   \n",
      "1  ID1012260530    2015-04-01   NSW            Lucerne           0.55   \n",
      "2  ID1025234388    2015-09-01    WA  SubcloverDalkeith           0.38   \n",
      "3  ID1028611175    2015-05-18   Tas           Ryegrass           0.66   \n",
      "4  ID1035947949    2015-09-11   Tas           Ryegrass           0.54   \n",
      "\n",
      "   Height_Ave_cm  Dry_Green_g  Dry_Dead_g  Dry_Clover_g    GDM_g  Dry_Total_g  \\\n",
      "0         4.6667      16.2751     31.9984        0.0000  16.2750      48.2735   \n",
      "1        16.0000       7.6000      0.0000        0.0000   7.6000       7.6000   \n",
      "2         1.0000       0.0000      0.0000        6.0500   6.0500       6.0500   \n",
      "3         5.0000      24.2376     30.9703        0.0000  24.2376      55.2079   \n",
      "4         3.5000      10.5261     23.2239        0.4343  10.9605      34.1844   \n",
      "\n",
      "   Year  Month  \n",
      "0  2015      9  \n",
      "1  2015      4  \n",
      "2  2015      9  \n",
      "3  2015      5  \n",
      "4  2015      9  \n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what columns are in train_wide\n",
    "print(\"train_wide columns:\", train_wide.columns.tolist())\n",
    "print(\"train_wide shape:\", train_wide.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(train_wide.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ca26b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Python Ridge models with GroupKFold...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Sampling_Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/utils/_indexing.py:443\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     col_idx = \u001b[43mall_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers.Integral):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'Sampling_Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m oof, models, calibrators\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTraining Python Ridge models with GroupKFold...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m python_oof, python_models, python_calibrators = \u001b[43mfit_predict_ridge_oof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_wide\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Apply physics constraints to OOF predictions\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m APPLY_PHYSICS_CONSTRAINTS:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mfit_predict_ridge_oof\u001b[39m\u001b[34m(train_wide)\u001b[39m\n\u001b[32m     53\u001b[39m y_tr = y_train[tr_idx]\n\u001b[32m     55\u001b[39m pipe = Pipeline([(\u001b[33m'\u001b[39m\u001b[33mprep\u001b[39m\u001b[33m'\u001b[39m, preprocess), (\u001b[33m'\u001b[39m\u001b[33mridge\u001b[39m\u001b[33m'\u001b[39m, Ridge(alpha=\u001b[32m1.0\u001b[39m, random_state=\u001b[32m42\u001b[39m))])\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m pred_va = pipe.predict(X_va)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Store raw predictions for calibration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:655\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    652\u001b[39m     )\n\u001b[32m    654\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:589\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    583\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    584\u001b[39m     step_idx=step_idx,\n\u001b[32m    585\u001b[39m     step_params=routed_params[name],\n\u001b[32m    586\u001b[39m     all_params=raw_params,\n\u001b[32m    587\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/joblib/memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:1540\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1543\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1544\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:988\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformers()\n\u001b[32m    986\u001b[39m n_samples = _num_samples(X)\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_remainder(X)\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:541\u001b[39m, in \u001b[36mColumnTransformer._validate_column_callables\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    539\u001b[39m         columns = columns(X)\n\u001b[32m    540\u001b[39m     all_columns.append(columns)\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     transformer_to_input_indices[name] = \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[38;5;28mself\u001b[39m._columns = all_columns\n\u001b[32m    544\u001b[39m \u001b[38;5;28mself\u001b[39m._transformer_to_input_indices = transformer_to_input_indices\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/utils/_indexing.py:451\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    448\u001b[39m         column_indices.append(col_idx)\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mA given column is not a column of the dataframe\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n",
      "\u001b[31mValueError\u001b[39m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Python baseline: Ridge with one-hot + scaling (GroupKFold by image)\n",
    "# Enhanced with log-space training and isotonic calibration\n",
    "# ===============================================================\n",
    "\n",
    "# Select features that exist\n",
    "feat_cols = [c for c in META_COLS if c in train_wide.columns]\n",
    "cat_cols  = [c for c in feat_cols if train_wide[c].dtype == 'object']\n",
    "num_cols  = [c for c in feat_cols if c not in cat_cols]\n",
    "\n",
    "# Simple date features if Sampling_Date exists\n",
    "if 'Sampling_Date' in feat_cols:\n",
    "    train_wide['Sampling_Date'] = pd.to_datetime(train_wide['Sampling_Date'], errors='coerce')\n",
    "    train_wide['Year']  = train_wide['Sampling_Date'].dt.year\n",
    "    train_wide['Month'] = train_wide['Sampling_Date'].dt.month\n",
    "    num_cols += ['Year','Month']\n",
    "    feat_cols = [c for c in feat_cols if c != 'Sampling_Date'] + ['Year','Month']\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), [c for c in num_cols if c in train_wide.columns]),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), [c for c in cat_cols if c in train_wide.columns])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "def fit_predict_ridge_oof(train_wide: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fit Ridge models with optional log-space training and isotonic calibration.\n",
    "    Returns OOF predictions and trained models (including calibrators).\n",
    "    \"\"\"\n",
    "    oof = pd.DataFrame({'image_id': train_wide['image_id'].values})\n",
    "    models = {}\n",
    "    calibrators = {} if USE_ISOTONIC_CALIBRATION else None\n",
    "    groups = train_wide['image_id']\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "    for t in TARGETS:\n",
    "        y = train_wide[t].values\n",
    "        \n",
    "        # Transform to log-space if configured\n",
    "        if USE_LOG_SPACE:\n",
    "            y_train = np.log1p(y)\n",
    "        else:\n",
    "            y_train = y\n",
    "            \n",
    "        oof_pred = np.zeros(len(train_wide), dtype=float)\n",
    "        oof_pred_raw = np.zeros(len(train_wide), dtype=float)  # For calibration\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(gkf.split(train_wide, y_train, groups=groups)):\n",
    "            X_tr = train_wide.iloc[tr_idx][feat_cols]\n",
    "            X_va = train_wide.iloc[va_idx][feat_cols]\n",
    "            y_tr = y_train[tr_idx]\n",
    "\n",
    "            pipe = Pipeline([('prep', preprocess), ('ridge', Ridge(alpha=1.0, random_state=42))])\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            pred_va = pipe.predict(X_va)\n",
    "            \n",
    "            # Store raw predictions for calibration\n",
    "            oof_pred_raw[va_idx] = pred_va\n",
    "\n",
    "        # Transform back from log-space\n",
    "        if USE_LOG_SPACE:\n",
    "            oof_pred_raw = np.expm1(oof_pred_raw)\n",
    "            oof_pred_raw = np.maximum(oof_pred_raw, 0)  # Clip negatives\n",
    "        \n",
    "        # Fit isotonic calibration on OOF predictions\n",
    "        if USE_ISOTONIC_CALIBRATION:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip')\n",
    "            iso.fit(oof_pred_raw, y)\n",
    "            oof_pred = iso.predict(oof_pred_raw)\n",
    "            calibrators[t] = iso\n",
    "        else:\n",
    "            oof_pred = oof_pred_raw\n",
    "\n",
    "        oof[t] = oof_pred\n",
    "        \n",
    "        # Fit final model on all data for test-time\n",
    "        if USE_LOG_SPACE:\n",
    "            y_all_train = np.log1p(y)\n",
    "        else:\n",
    "            y_all_train = y\n",
    "            \n",
    "        final_model = Pipeline([('prep', preprocess), ('ridge', Ridge(alpha=1.0, random_state=42))])\n",
    "        final_model.fit(train_wide[feat_cols], y_all_train)\n",
    "        models[t] = final_model\n",
    "        \n",
    "        print(f'âœ“ {t}: OOF RÂ² = {r2_manual(y, oof_pred):.4f}')\n",
    "\n",
    "    return oof, models, calibrators\n",
    "\n",
    "print('Training Python Ridge models with GroupKFold...')\n",
    "python_oof, python_models, python_calibrators = fit_predict_ridge_oof(train_wide)\n",
    "\n",
    "# Apply physics constraints to OOF predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    python_oof_constrained = apply_physical_constraints(python_oof[TARGETS])\n",
    "    python_oof[TARGETS] = python_oof_constrained\n",
    "\n",
    "# Evaluate Python-only OOF using long metric\n",
    "python_oof_long = preds_wide_to_long(train_wide['image_id'], python_oof[TARGETS])\n",
    "true_long = train[['sample_id','target_name','target']].copy()\n",
    "py_scores = weighted_r2_from_long(true_long, python_oof_long)\n",
    "print('\\nPython baseline weighted RÂ² (OOF):')\n",
    "print(json.dumps(py_scores, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# R modeling via rpy2 (if available)\n",
    "# ===============================================================\n",
    "have_r = False\n",
    "try:\n",
    "    get_ipython().run_line_magic('load_ext', 'rpy2.ipython')\n",
    "    have_r = True\n",
    "    print('rpy2.ipython extension loaded.')\n",
    "except Exception as e:\n",
    "    print('Could not load rpy2.ipython. R part will be skipped unless available.\\n', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ae8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data frames for R only if available\n",
    "if have_r:\n",
    "    # Build R-friendly train/test meta tables (wide targets + features)\n",
    "    r_train = train_wide[['image_id'] + [c for c in META_COLS if c in train_wide.columns] + TARGETS].copy()\n",
    "    r_test  = test_meta[['image_id'] + [c for c in META_COLS if c in test_meta.columns]].copy()\n",
    "\n",
    "    # If test lacks features, R model can't run; guard for that\n",
    "    r_can_predict_test = all(c in r_test.columns for c in META_COLS)\n",
    "    print('R can predict on test?', r_can_predict_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c26135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%R -i r_train -i r_test -o r_train_preds -o r_test_preds\n",
    "# Only runs if rpy2 loaded. Builds simple linear models per target.\n",
    "suppressPackageStartupMessages({\n",
    "  library(stats)\n",
    "})\n",
    "\n",
    "# Coerce date and create Year/Month if present\n",
    "date_cols <- intersect(colnames(r_train), c(\"Sampling_Date\"))\n",
    "if (length(date_cols) == 1) {\n",
    "  r_train$Sampling_Date <- as.Date(r_train$Sampling_Date)\n",
    "  r_train$Year <- as.integer(format(r_train$Sampling_Date, \"%Y\"))\n",
    "  r_train$Month <- as.integer(format(r_train$Sampling_Date, \"%m\"))\n",
    "}\n",
    "if (\"Sampling_Date\" %in% colnames(r_test)) {\n",
    "  r_test$Sampling_Date <- as.Date(r_test$Sampling_Date)\n",
    "  r_test$Year <- as.integer(format(r_test$Sampling_Date, \"%Y\"))\n",
    "  r_test$Month <- as.integer(format(r_test$Sampling_Date, \"%m\"))\n",
    "}\n",
    "\n",
    "# Feature set\n",
    "features <- c(\"Height_Ave_cm\",\"Pre_GSHH_NDVI\",\"State\",\"Species\",\"Year\",\"Month\")\n",
    "features <- intersect(features, colnames(r_train))\n",
    "\n",
    "predict_one <- function(target_name) {\n",
    "  rhs <- paste(features, collapse = \" + \")\n",
    "  frm <- as.formula(paste(target_name, \"~\", rhs))\n",
    "  mdl <- lm(frm, data = r_train)\n",
    "  pred_tr <- predict(mdl, newdata = r_train)\n",
    "  # For test, if features missing, return NAs\n",
    "  if (all(features %in% colnames(r_test))) {\n",
    "    pred_te <- predict(mdl, newdata = r_test)\n",
    "  } else {\n",
    "    pred_te <- rep(NA_real_, nrow(r_test))\n",
    "  }\n",
    "  list(tr = as.numeric(pred_tr), te = as.numeric(pred_te))\n",
    "}\n",
    "\n",
    "targets <- c(\"Dry_Green_g\",\"Dry_Dead_g\",\"Dry_Clover_g\",\"GDM_g\",\"Dry_Total_g\")\n",
    "r_train_preds <- data.frame(image_id = r_train$image_id)\n",
    "r_test_preds  <- data.frame(image_id = r_test$image_id)\n",
    "\n",
    "for (t in targets) {\n",
    "  res <- predict_one(t)\n",
    "  r_train_preds[[t]] <- res$tr\n",
    "  r_test_preds[[t]]  <- res$te\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba025d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Simple ensemble & evaluation\n",
    "# ===============================================================\n",
    "if have_r:\n",
    "    # Safe combine: if R preds missing (NA), fall back to Python\n",
    "    r_train_preds = r_train_preds\n",
    "    train_join = (python_oof.merge(r_train_preds, on='image_id', how='left', suffixes=('_py','_r')))\n",
    "    blend = pd.DataFrame({'image_id': train_join['image_id']})\n",
    "    for t in TARGETS:\n",
    "        a = train_join[f'{t}_py'].values\n",
    "        b = train_join[f'{t}_r'].values\n",
    "        b = np.where(np.isnan(b), a, b)  # replace NA with python preds\n",
    "        blend[t] = 0.5*a + 0.5*b\n",
    "\n",
    "    blend_long = preds_wide_to_long(train_join['image_id'], blend[TARGETS])\n",
    "    ens_scores = weighted_r2_from_long(true_long, blend_long)\n",
    "    print('Ensemble weighted R^2 (train OOF):', ens_scores)\n",
    "else:\n",
    "    print('Skipping R ensemble â€” rpy2 not available.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1417eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Final training on all data and test prediction\n",
    "# ===============================================================\n",
    "\n",
    "# Python final models already fit in python_models dict\n",
    "if HAVE_TEST_FEATURES:\n",
    "    # Prepare test feature matrix with same feature engineering\n",
    "    test_feat = test_meta.copy()\n",
    "    if 'Sampling_Date' in test_feat.columns:\n",
    "        test_feat['Sampling_Date'] = pd.to_datetime(test_feat['Sampling_Date'], errors='coerce')\n",
    "        test_feat['Year']  = test_feat['Sampling_Date'].dt.year\n",
    "        test_feat['Month'] = test_feat['Sampling_Date'].dt.month\n",
    "        test_feat = test_feat.drop(columns=['Sampling_Date'])\n",
    "    \n",
    "    # Predict with Python models\n",
    "    py_test_preds = pd.DataFrame({'image_id': test_feat['image_id'].values})\n",
    "    for t in TARGETS:\n",
    "        pred = python_models[t].predict(test_feat[[c for c in feat_cols if c != 'Sampling_Date']])\n",
    "        \n",
    "        # Transform back from log-space if needed\n",
    "        if USE_LOG_SPACE:\n",
    "            pred = np.expm1(pred)\n",
    "            pred = np.maximum(pred, 0)\n",
    "        \n",
    "        # Apply isotonic calibration if available\n",
    "        if USE_ISOTONIC_CALIBRATION and python_calibrators:\n",
    "            pred = python_calibrators[t].predict(pred)\n",
    "        \n",
    "        py_test_preds[t] = pred\n",
    "else:\n",
    "    # Fall back to per-target means\n",
    "    print('Warning: Test lacks features. Using per-target mean baseline.')\n",
    "    means = train.groupby('target_name')['target'].mean()\n",
    "    py_test_preds = pd.DataFrame({'image_id': sorted(test['image_id'].unique())})\n",
    "    for t in TARGETS:\n",
    "        py_test_preds[t] = float(means.get(t, train[train['target_name']==t]['target'].mean()))\n",
    "\n",
    "# If we have R test preds and they are valid, blend; else just Python\n",
    "final_preds_wide = py_test_preds.copy()\n",
    "if have_r:\n",
    "    # Align R test preds\n",
    "    r_te = r_test_preds.copy()\n",
    "    merged = final_preds_wide.merge(r_te, on='image_id', how='left', suffixes=('_py','_r'))\n",
    "    for t in TARGETS:\n",
    "        a = merged[f'{t}_py'].values\n",
    "        if f'{t}_r' in merged.columns:\n",
    "            b = merged[f'{t}_r'].values\n",
    "            if np.all(np.isnan(b)):\n",
    "                final_preds_wide[t] = a\n",
    "            else:\n",
    "                b = np.where(np.isnan(b), a, b)\n",
    "                final_preds_wide[t] = 0.5*a + 0.5*b\n",
    "        else:\n",
    "            final_preds_wide[t] = a\n",
    "    final_preds_wide = final_preds_wide[['image_id'] + TARGETS]\n",
    "\n",
    "# Apply physics constraints to final predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    print('Applying physics constraints to final predictions...')\n",
    "    final_preds_constrained = apply_physical_constraints(final_preds_wide[TARGETS])\n",
    "    final_preds_wide[TARGETS] = final_preds_constrained\n",
    "\n",
    "# Create submission\n",
    "sub_long = preds_wide_to_long(final_preds_wide['image_id'], final_preds_wide[TARGETS])\n",
    "submission = long_submission(sub_long)\n",
    "\n",
    "print('\\n=== Submission Preview ===')\n",
    "print(submission.head(10))\n",
    "print(f'\\nSubmission shape: {submission.shape}')\n",
    "print(f'Expected samples: {len(test[\"image_id\"].unique()) * 5}')\n",
    "print(f'\\nTarget statistics:')\n",
    "print(submission['target'].describe())\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('\\nâœ“ Wrote submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
