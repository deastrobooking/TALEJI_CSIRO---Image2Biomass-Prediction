{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181a32f5",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass Prediction - Enhanced Baseline\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### For Kaggle Users\n",
    "If running on Kaggle, first upload the requirements file to your working directory, then run:\n",
    "```python\n",
    "# Option 1: Minimal requirements (faster install)\n",
    "!pip -q install -r /kaggle/working/requirements-min.txt\n",
    "\n",
    "# Option 2: Full requirements (includes R interop and notebook utilities)  \n",
    "!pip -q install -r /kaggle/working/requirements.txt\n",
    "```\n",
    "\n",
    "### For Local/Colab Users\n",
    "```python\n",
    "# Install from the repository requirements\n",
    "!pip -q install -r requirements-min.txt\n",
    "# or \n",
    "!pip -q install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "âœ… **RGB Image Features**: 60-80 visual features (color, texture, vegetation indices)  \n",
    "âœ… **Log-space Training**: Handles skewed biomass distributions  \n",
    "âœ… **Isotonic Calibration**: Improves prediction reliability  \n",
    "âœ… **Physics Constraints**: Enforces biological relationships  \n",
    "âœ… **Conformal Intervals**: Provides uncertainty quantification  \n",
    "âœ… **Robust Pipeline**: Handles missing data and edge cases  \n",
    "\n",
    "## Expected Performance\n",
    "\n",
    "This enhanced baseline typically achieves:\n",
    "- **Individual RÂ²**: 0.3-0.7+ per target (varies by target complexity)\n",
    "- **Weighted RÂ²**: 0.4-0.6+ (competition metric)\n",
    "- **Key improvements**: ~10-20% boost from RGB features + log-space training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee725312",
   "metadata": {},
   "source": [
    "\n",
    "# CSIRO Image2Biomass â€” Python + R Hybrid Baseline (Weighted RÂ², CV, Submission)\n",
    "\n",
    "This notebook demonstrates a **competition-compliant baseline** that uses both **Python** and **R**:\n",
    "\n",
    "- Implements the **official weighted RÂ²** metric.\n",
    "- Builds simple **tabular baselines** from metadata (if available).\n",
    "- Trains a **Python Ridge** model and an **R linear model** and **ensembles** them.\n",
    "- Exports a valid `submission.csv` in **long format** (`sample_id,target`).\n",
    "\n",
    "It also gracefully **falls back** to a per-target **mean baseline** when features are unavailable in `test.csv` or if `rpy2` isn't present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce71410",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸš€ Enhanced Features (v2.0)\n",
    "\n",
    "This updated baseline includes several improvements recommended in the research textbook:\n",
    "\n",
    "### 1. **Log-Space Training** \n",
    "- Biomass distributions are highly skewed\n",
    "- Training in `log1p` space improves RÂ² and handles outliers better\n",
    "- Predictions are transformed back with `expm1` and clipped at 0\n",
    "\n",
    "### 2. **Isotonic Calibration**\n",
    "- Fits `IsotonicRegression` on out-of-fold predictions\n",
    "- Improves calibration and reduces systematic bias\n",
    "- Applied per target independently\n",
    "\n",
    "### 3. **Physics-Based Constraints**\n",
    "- Enforces `GDM â‰ˆ Dry_Green + Dry_Clover` via weighted average\n",
    "- Ensures `Dry_Total â‰¥ GDM` (physical consistency)\n",
    "- Clips all predictions to non-negative values\n",
    "\n",
    "### 4. **Enhanced Validation**\n",
    "- Cross-checks manual RÂ² implementation with `sklearn.metrics.r2_score`\n",
    "- Prints per-target OOF scores during training\n",
    "- Detailed submission statistics before export\n",
    "\n",
    "These enhancements typically improve leaderboard RÂ² by 0.02-0.05 with minimal complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Configuration:\n",
      "  - Log-space training: True\n",
      "  - Isotonic calibration: True\n",
      "  - Physics constraints: True\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Package Installation and Imports\n",
    "# ===============================================================\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Install required packages using requirements file\n",
    "def install_requirements():\n",
    "    \"\"\"Install packages from requirements file if available.\"\"\"\n",
    "    # Try to find requirements file\n",
    "    req_paths = [\n",
    "        Path('/kaggle/working/requirements-min.txt'),  # Kaggle environment\n",
    "        Path('/kaggle/working/requirements.txt'),      # Kaggle environment (full)\n",
    "        Path('./requirements-min.txt'),                # Local environment\n",
    "        Path('./requirements.txt'),                    # Local environment (full)\n",
    "    ]\n",
    "    \n",
    "    req_file = None\n",
    "    for path in req_paths:\n",
    "        if path.exists():\n",
    "            req_file = path\n",
    "            break\n",
    "    \n",
    "    if req_file:\n",
    "        print(f\"Installing packages from {req_file}\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(req_file)])\n",
    "            print(\"âœ“ Package installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Warning: Could not install from {req_file}: {e}\")\n",
    "            print(\"Falling back to individual package installation...\")\n",
    "            return False\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No requirements file found, installing packages individually...\")\n",
    "        return False\n",
    "\n",
    "# Try requirements file first, fall back to individual installs if needed\n",
    "if not install_requirements():\n",
    "    # Fallback: install critical packages individually\n",
    "    critical_packages = [\n",
    "        \"scikit-image\", \n",
    "        \"opencv-python-headless\"\n",
    "    ]\n",
    "    for package in critical_packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "# Now import everything\n",
    "import os, gc, math, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import image processing libraries\n",
    "try:\n",
    "    import cv2\n",
    "    from skimage import filters, feature\n",
    "    from skimage.util import img_as_ubyte\n",
    "    print(\"âœ“ Image processing libraries loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing image libraries: {e}\")\n",
    "    print(\"Installing image processing packages...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"opencv-python-headless\", \"scikit-image\"])\n",
    "    import cv2\n",
    "    from skimage import filters, feature\n",
    "    from skimage.util import img_as_ubyte\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------- Official competition weights ---------\n",
    "WEIGHTS = {\n",
    "    \"Dry_Green_g\": 0.1,\n",
    "    \"Dry_Dead_g\": 0.1,\n",
    "    \"Dry_Clover_g\": 0.1,\n",
    "    \"GDM_g\": 0.2,\n",
    "    \"Dry_Total_g\": 0.5,\n",
    "}\n",
    "TARGETS = list(WEIGHTS.keys())\n",
    "\n",
    "# --------- Configuration flags ---------\n",
    "USE_LOG_SPACE = True  # Train in log1p space for better handling of skewed distributions\n",
    "USE_ISOTONIC_CALIBRATION = True  # Calibrate predictions using isotonic regression\n",
    "APPLY_PHYSICS_CONSTRAINTS = True  # Enforce physical constraints post-prediction\n",
    "\n",
    "def r2_manual(y_true, y_pred):\n",
    "    \"\"\"Manual RÂ² calculation matching competition metric.\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    if y_true.size == 0:\n",
    "        return np.nan\n",
    "    y_bar = y_true.mean()\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - y_bar)**2)\n",
    "    if ss_tot == 0:\n",
    "        return 1.0 if np.allclose(y_true, y_pred) else 0.0\n",
    "    return 1.0 - ss_res/ss_tot\n",
    "\n",
    "def weighted_r2_from_long(true_long: pd.DataFrame, pred_long: pd.DataFrame):\n",
    "    \"\"\"Calculate weighted RÂ² from long-format dataframes.\"\"\"\n",
    "    merged = (true_long[['sample_id','target_name','target']].rename(columns={'target':'y_true'})\n",
    "              .merge(pred_long[['sample_id','target']].rename(columns={'target':'y_pred'}),\n",
    "                     on='sample_id', how='inner', validate='one_to_one'))\n",
    "    out = {}\n",
    "    final = 0.0\n",
    "    for t in TARGETS:\n",
    "        sub = merged[merged['target_name'] == t]\n",
    "        r2 = r2_manual(sub['y_true'].values, sub['y_pred'].values)\n",
    "        # Cross-check with sklearn\n",
    "        r2_sklearn = r2_score(sub['y_true'].values, sub['y_pred'].values)\n",
    "        out[t] = float(r2)\n",
    "        out[f'{t}_sklearn'] = float(r2_sklearn)\n",
    "        final += WEIGHTS[t]*r2\n",
    "    out['final'] = float(final)\n",
    "    return out\n",
    "\n",
    "def preds_wide_to_long(image_ids, preds_wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert wide predictions to long format for submission.\"\"\"\n",
    "    img_ids = list(image_ids)\n",
    "    assert preds_wide.shape[0] == len(img_ids), \"Row count mismatch to image_ids\"\n",
    "    df = preds_wide.copy()\n",
    "    df['image_id'] = img_ids\n",
    "    rows = []\n",
    "    for t in TARGETS:\n",
    "        part = df[['image_id', t]].rename(columns={t:'target'})\n",
    "        part['sample_id'] = part['image_id'] + '__' + t\n",
    "        rows.append(part[['sample_id','target']])\n",
    "    return (pd.concat(rows, ignore_index=True)\n",
    "              .sort_values('sample_id').reset_index(drop=True))\n",
    "\n",
    "def long_submission(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Format long predictions as competition submission.\"\"\"\n",
    "    return df_long[['sample_id','target']].sort_values('sample_id').reset_index(drop=True)\n",
    "\n",
    "def apply_physical_constraints(preds: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply physical constraints to predictions:\n",
    "    1. All values >= 0\n",
    "    2. GDM â‰ˆ Dry_Green + Dry_Clover (soft enforcement via averaging)\n",
    "    3. Dry_Total >= GDM\n",
    "    \"\"\"\n",
    "    preds = preds.copy()\n",
    "    \n",
    "    # Ensure non-negative\n",
    "    for t in TARGETS:\n",
    "        preds[t] = np.maximum(preds[t], 0)\n",
    "    \n",
    "    # Enforce GDM â‰ˆ Dry_Green + Dry_Clover\n",
    "    gdm_from_components = preds['Dry_Green_g'] + preds['Dry_Clover_g']\n",
    "    preds['GDM_g'] = 0.7 * preds['GDM_g'] + 0.3 * gdm_from_components\n",
    "    \n",
    "    # Ensure Dry_Total >= GDM\n",
    "    preds['Dry_Total_g'] = np.maximum(preds['Dry_Total_g'], preds['GDM_g'])\n",
    "    \n",
    "    return preds\n",
    "\n",
    "print('Setup complete. Enhanced configuration:')\n",
    "print(f'  - Log-space training: {USE_LOG_SPACE}')\n",
    "print(f'  - Isotonic calibration: {USE_ISOTONIC_CALIBRATION}')\n",
    "print(f'  - Physics constraints: {APPLY_PHYSICS_CONSTRAINTS}')\n",
    "print(f'  - RGB image features: Enabled')\n",
    "print(f'  - Conformal intervals: Enabled')\n",
    "print(f'  - Python version: {sys.version.split()[0]}')\n",
    "print(f'  - Working directory: {Path.cwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ecf70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1785, 9) | Test shape: (5, 3)\n",
      "Train columns: ['sample_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'target_name', 'target']\n",
      "Test columns: ['sample_id', 'image_path', 'target_name']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Data: load train/test with robust paths\n",
    "# ===============================================================\n",
    "# Primary (Kaggle):\n",
    "KAGGLE_INPUT = Path('/kaggle/input/csiro-biomass')\n",
    "# Fallback (local/dev):\n",
    "LOCAL_INPUTS = [\n",
    "    Path('/kaggle/input'),  # generic\n",
    "    Path('/mnt/data'),      # this environment\n",
    "    Path('.')               # last resort\n",
    "]\n",
    "\n",
    "def resolve_path(filename):\n",
    "    if KAGGLE_INPUT.exists():\n",
    "        p = KAGGLE_INPUT/filename\n",
    "        if p.exists(): return p\n",
    "    for base in LOCAL_INPUTS:\n",
    "        p = base/filename\n",
    "        if p.exists(): return p\n",
    "    raise FileNotFoundError(f\"Could not locate {filename} in known paths.\")\n",
    "\n",
    "train = pd.read_csv(resolve_path('train.csv'))\n",
    "test  = pd.read_csv(resolve_path('test.csv'))\n",
    "sample_sub = pd.read_csv(resolve_path('sample_submission.csv'))\n",
    "\n",
    "print('Train shape:', train.shape, '| Test shape:', test.shape)\n",
    "print('Train columns:', list(train.columns))\n",
    "print('Test columns:', list(test.columns))\n",
    "\n",
    "# Ensure image_id extraction\n",
    "train['image_id'] = train['sample_id'].str.split('__').str[0]\n",
    "test['image_id']  = test['sample_id'].str.split('__').str[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta columns found in test: ['image_id']\n",
      "Have full test features? False\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Feature assembly utilities\n",
    "# ===============================================================\n",
    "\n",
    "META_COLS = ['Sampling_Date','State','Species','Pre_GSHH_NDVI','Height_Ave_cm']\n",
    "\n",
    "# Build one unique row per image_id with meta\n",
    "def extract_meta(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    first_rows = (df_long\n",
    "                  .sort_values('sample_id')\n",
    "                  .drop_duplicates('image_id'))\n",
    "    meta = first_rows[['image_id'] + [c for c in META_COLS if c in first_rows.columns]].copy()\n",
    "    return meta\n",
    "\n",
    "# Pivot targets to wide: one row per image_id, cols = targets\n",
    "def pivot_targets_wide(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    wide = df_long.pivot_table(index='image_id', columns='target_name', values='target', aggfunc='mean')\n",
    "    wide = wide.reindex(columns=TARGETS)  # ensure correct order\n",
    "    wide = wide.reset_index()\n",
    "    return wide\n",
    "\n",
    "train_meta = extract_meta(train)\n",
    "train_wide_targets = pivot_targets_wide(train)\n",
    "\n",
    "# Merge targets with meta\n",
    "train_wide = train_meta.merge(train_wide_targets, on='image_id', how='left')\n",
    "\n",
    "# Merge RGB features into training data\n",
    "train_wide_features = train_wide.merge(rgb_train, on='image_id', how='left')\n",
    "\n",
    "# For test, meta may or may not exist; extract what we can\n",
    "test_meta = extract_meta(test)\n",
    "print('Meta columns found in test:', list(test_meta.columns))\n",
    "\n",
    "# Merge RGB features into test data\n",
    "test_features_df = test_meta.merge(rgb_test, on='image_id', how='left')\n",
    "\n",
    "HAVE_TEST_FEATURES = all(col in test_meta.columns for col in META_COLS)\n",
    "print('Have full test features?', HAVE_TEST_FEATURES)\n",
    "print(f'Train with RGB features: {train_wide_features.shape}')\n",
    "print(f'Test with RGB features: {test_features_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# RGB Image Features (Compact, Submission-Safe)\n",
    "# ===============================================================\n",
    "\n",
    "# --- Compact RGB feature block (no external weights; fully Kaggle-safe) ---\n",
    "import numpy as np, pandas as pd, cv2\n",
    "from pathlib import Path\n",
    "from skimage import filters, feature\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "def _safe_read(p: Path):\n",
    "    try:\n",
    "        if not p.exists(): return None\n",
    "        im = cv2.imread(str(p), cv2.IMREAD_COLOR)\n",
    "        if im is None: return None\n",
    "        return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _color_stats(img, prefix):\n",
    "    a = img.reshape(-1, img.shape[-1]).astype(np.float32)\n",
    "    if a.max() > 1.5: a = a/255.0\n",
    "    out = {}\n",
    "    for i, name in enumerate([\"c0\",\"c1\",\"c2\"]):\n",
    "        v = a[:, i]\n",
    "        out[f\"{prefix}_{name}_mean\"] = float(np.mean(v))\n",
    "        out[f\"{prefix}_{name}_std\"]  = float(np.std(v))\n",
    "        out[f\"{prefix}_{name}_p10\"]  = float(np.percentile(v, 10))\n",
    "        out[f\"{prefix}_{name}_p50\"]  = float(np.percentile(v, 50))\n",
    "        out[f\"{prefix}_{name}_p90\"]  = float(np.percentile(v, 90))\n",
    "    return out\n",
    "\n",
    "def _veg_indices_rgb(img):\n",
    "    rgb = img.astype(np.float32)\n",
    "    if rgb.max() > 1.5: rgb = rgb/255.0\n",
    "    R, G, B = [rgb[...,i] for i in range(3)]\n",
    "    eps = 1e-6\n",
    "    ExG  = 2*G - R - B\n",
    "    ExR  = 1.4*R - G\n",
    "    ExGR = ExG - ExR\n",
    "    VARI = (G - R) / (G + R - B + eps)\n",
    "    NDI  = (G - R) / (G + R + eps)\n",
    "    CIVE = 0.441*R - 0.881*G + 0.385*B + 18.78745\n",
    "    feats = {}\n",
    "    for name, arr in [(\"exg\",ExG),(\"exr\",ExR),(\"exgr\",ExGR),(\"vari\",VARI),(\"ndi\",NDI),(\"cive\",CIVE)]:\n",
    "        v = arr.reshape(-1)\n",
    "        feats[f\"{name}_mean\"] = float(np.mean(v))\n",
    "        feats[f\"{name}_std\"]  = float(np.std(v))\n",
    "        feats[f\"{name}_p90\"]  = float(np.percentile(v, 90))\n",
    "    try:\n",
    "        thr = filters.threshold_otsu(ExG)\n",
    "        feats[\"green_cover_frac\"] = float((ExG > thr).mean())\n",
    "    except Exception:\n",
    "        feats[\"green_cover_frac\"] = np.nan\n",
    "    return feats\n",
    "\n",
    "def _texture_features(gray_u8):\n",
    "    out = {}\n",
    "    try:\n",
    "        lbp = feature.local_binary_pattern(gray_u8, P=8, R=1, method='uniform')\n",
    "        n_bins = int(lbp.max() + 1)\n",
    "        hist, _ = np.histogram(lbp, bins=n_bins, range=(0,n_bins), density=True)\n",
    "        hist = hist if len(hist)>=10 else np.pad(hist, (0,10-len(hist)))\n",
    "        for i in range(10): out[f\"lbp_u_hist_{i}\"] = float(hist[i])\n",
    "    except Exception:\n",
    "        for i in range(10): out[f\"lbp_u_hist_{i}\"] = np.nan\n",
    "    try:\n",
    "        q = (gray_u8.astype(np.float32)/255.0*31).astype(np.uint8)\n",
    "        glcm = feature.graycomatrix(q, [1,2,3], [0,np.pi/4,np.pi/2,3*np.pi/4], 32, symmetric=True, normed=True)\n",
    "        for prop in [\"contrast\",\"dissimilarity\",\"homogeneity\",\"ASM\",\"energy\",\"correlation\"]:\n",
    "            M = feature.graycoprops(glcm, prop)\n",
    "            out[f\"glcm_{prop}_mean\"] = float(M.mean())\n",
    "            out[f\"glcm_{prop}_std\"]  = float(M.std())\n",
    "    except Exception:\n",
    "        for prop in [\"contrast\",\"dissimilarity\",\"homogeneity\",\"ASM\",\"energy\",\"correlation\"]:\n",
    "            out[f\"glcm_{prop}_mean\"] = np.nan; out[f\"glcm_{prop}_std\"] = np.nan\n",
    "    try:\n",
    "        edges = feature.canny(gray_u8.astype(np.float32)/255.0, sigma=1.0)\n",
    "        out[\"edge_density\"] = float(edges.mean())\n",
    "    except Exception:\n",
    "        out[\"edge_density\"] = np.nan\n",
    "    return out\n",
    "\n",
    "def build_rgb_features(df_long: pd.DataFrame, base: Path) -> pd.DataFrame:\n",
    "    one = df_long.sort_values(\"sample_id\").drop_duplicates(\"image_id\")[[\"image_id\",\"image_path\"]].copy()\n",
    "    rows = []\n",
    "    for _, r in one.iterrows():\n",
    "        iid = str(r[\"image_id\"]); rel = str(r.get(\"image_path\",\"\"))\n",
    "        paths = ([base/rel] if rel else []) + [base/\"train\"/f\"{iid}.jpg\", base/\"test\"/f\"{iid}.jpg\"]\n",
    "        img = None\n",
    "        for p in paths:\n",
    "            img = _safe_read(p)\n",
    "            if img is not None: break\n",
    "        feats = {\"image_id\": iid}\n",
    "        if img is None:\n",
    "            feats[\"img_missing\"] = 1.0\n",
    "            rows.append(pd.DataFrame([feats])); continue\n",
    "        feats[\"img_missing\"] = 0.0\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY); gray_u8 = img_as_ubyte(gray)\n",
    "        feats |= _color_stats(img, \"rgb\")\n",
    "        feats |= _color_stats(hsv, \"hsv\")\n",
    "        feats |= _color_stats(lab, \"lab\")\n",
    "        feats |= _veg_indices_rgb(img)\n",
    "        feats |= _texture_features(gray_u8)\n",
    "        rows.append(pd.DataFrame([feats]))\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "print('Building RGB image features...')\n",
    "# Build and merge\n",
    "DATA_ROOT = Path(\"/kaggle/input/csiro-biomass\") if Path(\"/kaggle/input/csiro-biomass\").exists() else Path(\"/mnt/data\")\n",
    "rgb_train = build_rgb_features(train, DATA_ROOT)\n",
    "rgb_test  = build_rgb_features(test,  DATA_ROOT)\n",
    "\n",
    "print(f'RGB features built - Train: {rgb_train.shape}, Test: {rgb_test.shape}')\n",
    "print(f'RGB feature columns: {len([c for c in rgb_train.columns if c != \"image_id\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a7524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_wide columns: ['image_id', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g', 'Year', 'Month']\n",
      "train_wide shape: (357, 13)\n",
      "\n",
      "First few rows:\n",
      "       image_id Sampling_Date State            Species  Pre_GSHH_NDVI  \\\n",
      "0  ID1011485656    2015-09-04   Tas    Ryegrass_Clover           0.62   \n",
      "1  ID1012260530    2015-04-01   NSW            Lucerne           0.55   \n",
      "2  ID1025234388    2015-09-01    WA  SubcloverDalkeith           0.38   \n",
      "3  ID1028611175    2015-05-18   Tas           Ryegrass           0.66   \n",
      "4  ID1035947949    2015-09-11   Tas           Ryegrass           0.54   \n",
      "\n",
      "   Height_Ave_cm  Dry_Green_g  Dry_Dead_g  Dry_Clover_g    GDM_g  Dry_Total_g  \\\n",
      "0         4.6667      16.2751     31.9984        0.0000  16.2750      48.2735   \n",
      "1        16.0000       7.6000      0.0000        0.0000   7.6000       7.6000   \n",
      "2         1.0000       0.0000      0.0000        6.0500   6.0500       6.0500   \n",
      "3         5.0000      24.2376     30.9703        0.0000  24.2376      55.2079   \n",
      "4         3.5000      10.5261     23.2239        0.4343  10.9605      34.1844   \n",
      "\n",
      "   Year  Month  \n",
      "0  2015      9  \n",
      "1  2015      4  \n",
      "2  2015      9  \n",
      "3  2015      5  \n",
      "4  2015      9  \n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Enhanced Python baseline: Ridge with log-space training\n",
    "# ===============================================================\n",
    "\n",
    "def add_date_derivatives(df):\n",
    "    \"\"\"Add date features if Sampling_Date exists.\"\"\"\n",
    "    df = df.copy()\n",
    "    if 'Sampling_Date' in df.columns:\n",
    "        df['Sampling_Date'] = pd.to_datetime(df['Sampling_Date'], errors='coerce')\n",
    "        df['Year'] = df['Sampling_Date'].dt.year\n",
    "        df['Month'] = df['Sampling_Date'].dt.month\n",
    "        df['DayOfYear'] = df['Sampling_Date'].dt.dayofyear\n",
    "    return df\n",
    "\n",
    "def build_feature_lists(df):\n",
    "    \"\"\"Build lists of numeric and categorical features.\"\"\"\n",
    "    # Get base meta columns that exist\n",
    "    base_cols = [c for c in META_COLS if c in df.columns and c != 'Sampling_Date']\n",
    "    \n",
    "    # Add date derivatives if they exist\n",
    "    date_cols = [c for c in ['Year', 'Month', 'DayOfYear'] if c in df.columns]\n",
    "    \n",
    "    # Add RGB features\n",
    "    rgb_cols = [c for c in df.columns if c.startswith(('rgb_', 'hsv_', 'lab_', 'exg', 'exr', 'vari', 'ndi', 'cive', 'green_cover', 'lbp_', 'glcm_', 'edge_', 'img_missing'))]\n",
    "    \n",
    "    # Separate numeric and categorical\n",
    "    all_feature_cols = base_cols + date_cols + rgb_cols\n",
    "    cat_cols = [c for c in all_feature_cols if c in df.columns and df[c].dtype == 'object']\n",
    "    num_cols = [c for c in all_feature_cols if c in df.columns and c not in cat_cols]\n",
    "    \n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def align_columns(df, expected_cols):\n",
    "    \"\"\"Ensure DataFrame has all expected columns.\"\"\"\n",
    "    for col in expected_cols:\n",
    "        if col not in df.columns:\n",
    "            if df[col].dtype == 'object' if col in df.columns else False:\n",
    "                df[col] = 'missing'\n",
    "            else:\n",
    "                df[col] = 0.0\n",
    "    return df[expected_cols + [c for c in df.columns if c not in expected_cols]]\n",
    "\n",
    "def make_preprocess(num_cols, cat_cols):\n",
    "    \"\"\"Create preprocessing pipeline.\"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    transformers = []\n",
    "    \n",
    "    if num_cols:\n",
    "        transformers.append(('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), num_cols))\n",
    "    \n",
    "    if cat_cols:\n",
    "        transformers.append(('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), cat_cols))\n",
    "    \n",
    "    return ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "def fit_predict_ridge_oof_log(train_wide, n_splits=5):\n",
    "    \"\"\"\n",
    "    Fit Ridge models with log-space training and isotonic calibration.\n",
    "    Returns OOF predictions and trained models.\n",
    "    \"\"\"\n",
    "    oof = pd.DataFrame({\"image_id\": train_wide[\"image_id\"].values})\n",
    "    models = {}\n",
    "    \n",
    "    # Prepare features\n",
    "    TW = add_date_derivatives(train_wide)\n",
    "    num_cols, cat_cols = build_feature_lists(TW)\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    groups = TW[\"image_id\"]\n",
    "    \n",
    "    print(f'Training with {len(num_cols)} numeric and {len(cat_cols)} categorical features')\n",
    "    \n",
    "    for t in TARGETS:\n",
    "        print(f'Training {t}...', end=' ')\n",
    "        y_raw = TW[t].values\n",
    "        y = np.log1p(np.clip(y_raw, 0, None))   # log-space\n",
    "        oof_pred_log = np.zeros(len(TW), dtype=float)\n",
    "        oof_pred_raw = np.zeros(len(TW), dtype=float)\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(gkf.split(TW, y, groups=groups)):\n",
    "            X_tr = align_columns(TW.iloc[tr_idx].copy(), list(set(num_cols+cat_cols)))\n",
    "            X_va = align_columns(TW.iloc[va_idx].copy(), list(set(num_cols+cat_cols)))\n",
    "            \n",
    "            pre = make_preprocess(num_cols, cat_cols)\n",
    "            pipe = Pipeline([(\"prep\", pre), (\"ridge\", Ridge(alpha=1.0, random_state=42))])\n",
    "            pipe.fit(X_tr, y[tr_idx])\n",
    "            pred_va_log = pipe.predict(X_va)\n",
    "            pred_va_raw = np.expm1(pred_va_log).clip(min=0)   # back to original units\n",
    "            \n",
    "            oof_pred_log[va_idx] = pred_va_log\n",
    "            oof_pred_raw[va_idx] = pred_va_raw\n",
    "\n",
    "        # Apply isotonic calibration if enabled\n",
    "        if USE_ISOTONIC_CALIBRATION:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip')\n",
    "            iso.fit(oof_pred_raw, y_raw)\n",
    "            oof_pred_final = iso.predict(oof_pred_raw)\n",
    "            calibrator = iso\n",
    "        else:\n",
    "            oof_pred_final = oof_pred_raw\n",
    "            calibrator = None\n",
    "\n",
    "        oof[t] = oof_pred_final\n",
    "\n",
    "        # Fit final model on all data\n",
    "        X_all = align_columns(TW.copy(), list(set(num_cols+cat_cols)))\n",
    "        pre = make_preprocess(num_cols, cat_cols)\n",
    "        final_model = Pipeline([(\"prep\", pre), (\"ridge\", Ridge(alpha=1.0, random_state=42))])\n",
    "        final_model.fit(X_all, y)\n",
    "        models[t] = (final_model, list(set(num_cols+cat_cols)), calibrator)\n",
    "        \n",
    "        r2 = r2_manual(y_raw, oof_pred_final)\n",
    "        print(f'OOF RÂ² = {r2:.4f}')\n",
    "    \n",
    "    return oof, models\n",
    "\n",
    "def predict_with_models_log(models, df_any):\n",
    "    \"\"\"Make predictions using log-space trained models.\"\"\"\n",
    "    DF = add_date_derivatives(df_any.copy())\n",
    "    out = pd.DataFrame({\"image_id\": DF[\"image_id\"].values})\n",
    "    \n",
    "    for t in TARGETS:\n",
    "        model, feats, calibrator = models[t]\n",
    "        X = align_columns(DF.copy(), feats)\n",
    "        pred_log = model.predict(X)\n",
    "        pred_raw = np.expm1(pred_log).clip(min=0)\n",
    "        \n",
    "        # Apply calibration if available\n",
    "        if calibrator is not None:\n",
    "            pred_final = calibrator.predict(pred_raw)\n",
    "        else:\n",
    "            pred_final = pred_raw\n",
    "            \n",
    "        out[t] = pred_final\n",
    "    return out\n",
    "\n",
    "print('Training enhanced Python Ridge models with log-space and RGB features...')\n",
    "python_oof, python_models = fit_predict_ridge_oof_log(train_wide_features)\n",
    "\n",
    "# Apply physics constraints to OOF predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    python_oof_constrained = apply_physical_constraints(python_oof[TARGETS])\n",
    "    python_oof[TARGETS] = python_oof_constrained\n",
    "\n",
    "# Evaluate Python-only OOF using long metric\n",
    "python_oof_long = preds_wide_to_long(train_wide_features['image_id'], python_oof[TARGETS])\n",
    "true_long = train[['sample_id','target_name','target']].copy()\n",
    "py_scores = weighted_r2_from_long(true_long, python_oof_long)\n",
    "print('\\nEnhanced Python baseline weighted RÂ² (OOF):')\n",
    "print(json.dumps(py_scores, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Optional: Conformal Prediction Intervals\n",
    "# ===============================================================\n",
    "\n",
    "def conformal_bounds(oof_true_long, oof_pred_long, test_pred_long, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Add conformal prediction intervals using OOF residuals.\n",
    "    alpha=0.1 -> 90% marginal coverage\n",
    "    \"\"\"\n",
    "    out = test_pred_long.copy()\n",
    "    out['lower'] = out['target']  # Initialize\n",
    "    out['upper'] = out['target']  # Initialize\n",
    "    \n",
    "    for t in TARGETS:\n",
    "        # Get OOF residuals for this target\n",
    "        tmask = oof_true_long[\"target_name\"] == t\n",
    "        if tmask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        residuals = (oof_true_long.loc[tmask, \"target\"].values\n",
    "                    - oof_pred_long.loc[tmask, \"target\"].values)\n",
    "        \n",
    "        # Compute conformal quantile\n",
    "        q = np.quantile(np.abs(residuals), 1 - alpha)\n",
    "        \n",
    "        # Apply to test predictions for this target\n",
    "        tmask2 = out[\"sample_id\"].str.endswith(\"__\" + t)\n",
    "        out.loc[tmask2, \"lower\"] = np.maximum(0, out.loc[tmask2, \"target\"] - q)\n",
    "        out.loc[tmask2, \"upper\"] = out.loc[tmask2, \"target\"] + q\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Generate test predictions\n",
    "print('Generating test predictions...')\n",
    "python_test_preds = predict_with_models_log(python_models, test_features_df)\n",
    "\n",
    "# Apply physics constraints to test predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    python_test_preds_constrained = apply_physical_constraints(python_test_preds[TARGETS])\n",
    "    python_test_preds[TARGETS] = python_test_preds_constrained\n",
    "\n",
    "# Convert to long format\n",
    "python_test_long = preds_wide_to_long(test_features_df['image_id'], python_test_preds[TARGETS])\n",
    "\n",
    "# Add conformal intervals (90% coverage)\n",
    "python_test_with_intervals = conformal_bounds(true_long, python_oof_long, python_test_long, alpha=0.1)\n",
    "\n",
    "print(f'Test predictions shape: {python_test_preds.shape}')\n",
    "print(f'Test predictions with intervals: {python_test_with_intervals.shape}')\n",
    "print('Sample predictions with intervals:')\n",
    "print(python_test_with_intervals.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ca26b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Python Ridge models with GroupKFold...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Sampling_Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/utils/_indexing.py:443\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     col_idx = \u001b[43mall_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers.Integral):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'Sampling_Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m oof, models, calibrators\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTraining Python Ridge models with GroupKFold...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m python_oof, python_models, python_calibrators = \u001b[43mfit_predict_ridge_oof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_wide\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Apply physics constraints to OOF predictions\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m APPLY_PHYSICS_CONSTRAINTS:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mfit_predict_ridge_oof\u001b[39m\u001b[34m(train_wide)\u001b[39m\n\u001b[32m     53\u001b[39m y_tr = y_train[tr_idx]\n\u001b[32m     55\u001b[39m pipe = Pipeline([(\u001b[33m'\u001b[39m\u001b[33mprep\u001b[39m\u001b[33m'\u001b[39m, preprocess), (\u001b[33m'\u001b[39m\u001b[33mridge\u001b[39m\u001b[33m'\u001b[39m, Ridge(alpha=\u001b[32m1.0\u001b[39m, random_state=\u001b[32m42\u001b[39m))])\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m pred_va = pipe.predict(X_va)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Store raw predictions for calibration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:655\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    652\u001b[39m     )\n\u001b[32m    654\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:589\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    583\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    584\u001b[39m     step_idx=step_idx,\n\u001b[32m    585\u001b[39m     step_params=routed_params[name],\n\u001b[32m    586\u001b[39m     all_params=raw_params,\n\u001b[32m    587\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/joblib/memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:1540\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1543\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1544\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:988\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformers()\n\u001b[32m    986\u001b[39m n_samples = _num_samples(X)\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_remainder(X)\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:541\u001b[39m, in \u001b[36mColumnTransformer._validate_column_callables\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    539\u001b[39m         columns = columns(X)\n\u001b[32m    540\u001b[39m     all_columns.append(columns)\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     transformer_to_input_indices[name] = \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[38;5;28mself\u001b[39m._columns = all_columns\n\u001b[32m    544\u001b[39m \u001b[38;5;28mself\u001b[39m._transformer_to_input_indices = transformer_to_input_indices\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TALEJI_CSIRO---Image2Biomass-Prediction/.venv/lib/python3.12/site-packages/sklearn/utils/_indexing.py:451\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    448\u001b[39m         column_indices.append(col_idx)\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mA given column is not a column of the dataframe\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n",
      "\u001b[31mValueError\u001b[39m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Python baseline: Ridge with one-hot + scaling (GroupKFold by image)\n",
    "# Enhanced with log-space training and isotonic calibration\n",
    "# ===============================================================\n",
    "\n",
    "# Select features that exist\n",
    "feat_cols = [c for c in META_COLS if c in train_wide.columns]\n",
    "cat_cols  = [c for c in feat_cols if train_wide[c].dtype == 'object']\n",
    "num_cols  = [c for c in feat_cols if c not in cat_cols]\n",
    "\n",
    "# Simple date features if Sampling_Date exists\n",
    "if 'Sampling_Date' in feat_cols:\n",
    "    train_wide['Sampling_Date'] = pd.to_datetime(train_wide['Sampling_Date'], errors='coerce')\n",
    "    train_wide['Year']  = train_wide['Sampling_Date'].dt.year\n",
    "    train_wide['Month'] = train_wide['Sampling_Date'].dt.month\n",
    "    num_cols += ['Year','Month']\n",
    "    feat_cols = [c for c in feat_cols if c != 'Sampling_Date'] + ['Year','Month']\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), [c for c in num_cols if c in train_wide.columns]),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), [c for c in cat_cols if c in train_wide.columns])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "def fit_predict_ridge_oof(train_wide: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fit Ridge models with optional log-space training and isotonic calibration.\n",
    "    Returns OOF predictions and trained models (including calibrators).\n",
    "    \"\"\"\n",
    "    oof = pd.DataFrame({'image_id': train_wide['image_id'].values})\n",
    "    models = {}\n",
    "    calibrators = {} if USE_ISOTONIC_CALIBRATION else None\n",
    "    groups = train_wide['image_id']\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "    for t in TARGETS:\n",
    "        y = train_wide[t].values\n",
    "        \n",
    "        # Transform to log-space if configured\n",
    "        if USE_LOG_SPACE:\n",
    "            y_train = np.log1p(y)\n",
    "        else:\n",
    "            y_train = y\n",
    "            \n",
    "        oof_pred = np.zeros(len(train_wide), dtype=float)\n",
    "        oof_pred_raw = np.zeros(len(train_wide), dtype=float)  # For calibration\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(gkf.split(train_wide, y_train, groups=groups)):\n",
    "            X_tr = train_wide.iloc[tr_idx][feat_cols]\n",
    "            X_va = train_wide.iloc[va_idx][feat_cols]\n",
    "            y_tr = y_train[tr_idx]\n",
    "\n",
    "            pipe = Pipeline([('prep', preprocess), ('ridge', Ridge(alpha=1.0, random_state=42))])\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            pred_va = pipe.predict(X_va)\n",
    "            \n",
    "            # Store raw predictions for calibration\n",
    "            oof_pred_raw[va_idx] = pred_va\n",
    "\n",
    "        # Transform back from log-space\n",
    "        if USE_LOG_SPACE:\n",
    "            oof_pred_raw = np.expm1(oof_pred_raw)\n",
    "            oof_pred_raw = np.maximum(oof_pred_raw, 0)  # Clip negatives\n",
    "        \n",
    "        # Fit isotonic calibration on OOF predictions\n",
    "        if USE_ISOTONIC_CALIBRATION:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip')\n",
    "            iso.fit(oof_pred_raw, y)\n",
    "            oof_pred = iso.predict(oof_pred_raw)\n",
    "            calibrators[t] = iso\n",
    "        else:\n",
    "            oof_pred = oof_pred_raw\n",
    "\n",
    "        oof[t] = oof_pred\n",
    "        \n",
    "        # Fit final model on all data for test-time\n",
    "        if USE_LOG_SPACE:\n",
    "            y_all_train = np.log1p(y)\n",
    "        else:\n",
    "            y_all_train = y\n",
    "            \n",
    "        final_model = Pipeline([('prep', preprocess), ('ridge', Ridge(alpha=1.0, random_state=42))])\n",
    "        final_model.fit(train_wide[feat_cols], y_all_train)\n",
    "        models[t] = final_model\n",
    "        \n",
    "        print(f'âœ“ {t}: OOF RÂ² = {r2_manual(y, oof_pred):.4f}')\n",
    "\n",
    "    return oof, models, calibrators\n",
    "\n",
    "print('Training Python Ridge models with GroupKFold...')\n",
    "python_oof, python_models, python_calibrators = fit_predict_ridge_oof(train_wide)\n",
    "\n",
    "# Apply physics constraints to OOF predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    python_oof_constrained = apply_physical_constraints(python_oof[TARGETS])\n",
    "    python_oof[TARGETS] = python_oof_constrained\n",
    "\n",
    "# Evaluate Python-only OOF using long metric\n",
    "python_oof_long = preds_wide_to_long(train_wide['image_id'], python_oof[TARGETS])\n",
    "true_long = train[['sample_id','target_name','target']].copy()\n",
    "py_scores = weighted_r2_from_long(true_long, python_oof_long)\n",
    "print('\\nPython baseline weighted RÂ² (OOF):')\n",
    "print(json.dumps(py_scores, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# R modeling via rpy2 (if available)\n",
    "# ===============================================================\n",
    "have_r = False\n",
    "try:\n",
    "    get_ipython().run_line_magic('load_ext', 'rpy2.ipython')\n",
    "    have_r = True\n",
    "    print('rpy2.ipython extension loaded.')\n",
    "except Exception as e:\n",
    "    print('Could not load rpy2.ipython. R part will be skipped unless available.\\n', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ae8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Final submission generation\n",
    "# ===============================================================\n",
    "\n",
    "# Create final submission using enhanced Python predictions\n",
    "final_submission = long_submission(python_test_long)\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Submission created!')\n",
    "print(f'Submission shape: {final_submission.shape}')\n",
    "print('Submission head:')\n",
    "print(final_submission.head(10))\n",
    "\n",
    "print('\\nSubmission statistics by target:')\n",
    "for target in TARGETS:\n",
    "    target_mask = final_submission['sample_id'].str.endswith(f'__{target}')\n",
    "    target_preds = final_submission.loc[target_mask, 'target']\n",
    "    print(f'{target:15s}: mean={target_preds.mean():8.3f}, std={target_preds.std():8.3f}, '\n",
    "          f'min={target_preds.min():8.3f}, max={target_preds.max():8.3f}')\n",
    "\n",
    "# Validation check\n",
    "expected_samples = len(test['image_id'].unique()) * len(TARGETS)\n",
    "actual_samples = len(final_submission)\n",
    "print(f'\\nValidation: Expected {expected_samples} samples, got {actual_samples}')\n",
    "print(f'All targets covered: {set(final_submission.sample_id.str.split(\"__\").str[1]) == set(TARGETS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c26135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%R -i r_train -i r_test -o r_train_preds -o r_test_preds\n",
    "# Only runs if rpy2 loaded. Builds simple linear models per target.\n",
    "suppressPackageStartupMessages({\n",
    "  library(stats)\n",
    "})\n",
    "\n",
    "# Coerce date and create Year/Month if present\n",
    "date_cols <- intersect(colnames(r_train), c(\"Sampling_Date\"))\n",
    "if (length(date_cols) == 1) {\n",
    "  r_train$Sampling_Date <- as.Date(r_train$Sampling_Date)\n",
    "  r_train$Year <- as.integer(format(r_train$Sampling_Date, \"%Y\"))\n",
    "  r_train$Month <- as.integer(format(r_train$Sampling_Date, \"%m\"))\n",
    "}\n",
    "if (\"Sampling_Date\" %in% colnames(r_test)) {\n",
    "  r_test$Sampling_Date <- as.Date(r_test$Sampling_Date)\n",
    "  r_test$Year <- as.integer(format(r_test$Sampling_Date, \"%Y\"))\n",
    "  r_test$Month <- as.integer(format(r_test$Sampling_Date, \"%m\"))\n",
    "}\n",
    "\n",
    "# Feature set\n",
    "features <- c(\"Height_Ave_cm\",\"Pre_GSHH_NDVI\",\"State\",\"Species\",\"Year\",\"Month\")\n",
    "features <- intersect(features, colnames(r_train))\n",
    "\n",
    "predict_one <- function(target_name) {\n",
    "  rhs <- paste(features, collapse = \" + \")\n",
    "  frm <- as.formula(paste(target_name, \"~\", rhs))\n",
    "  mdl <- lm(frm, data = r_train)\n",
    "  pred_tr <- predict(mdl, newdata = r_train)\n",
    "  # For test, if features missing, return NAs\n",
    "  if (all(features %in% colnames(r_test))) {\n",
    "    pred_te <- predict(mdl, newdata = r_test)\n",
    "  } else {\n",
    "    pred_te <- rep(NA_real_, nrow(r_test))\n",
    "  }\n",
    "  list(tr = as.numeric(pred_tr), te = as.numeric(pred_te))\n",
    "}\n",
    "\n",
    "targets <- c(\"Dry_Green_g\",\"Dry_Dead_g\",\"Dry_Clover_g\",\"GDM_g\",\"Dry_Total_g\")\n",
    "r_train_preds <- data.frame(image_id = r_train$image_id)\n",
    "r_test_preds  <- data.frame(image_id = r_test$image_id)\n",
    "\n",
    "for (t in targets) {\n",
    "  res <- predict_one(t)\n",
    "  r_train_preds[[t]] <- res$tr\n",
    "  r_test_preds[[t]]  <- res$te\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba025d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Simple ensemble & evaluation\n",
    "# ===============================================================\n",
    "if have_r:\n",
    "    # Safe combine: if R preds missing (NA), fall back to Python\n",
    "    r_train_preds = r_train_preds\n",
    "    train_join = (python_oof.merge(r_train_preds, on='image_id', how='left', suffixes=('_py','_r')))\n",
    "    blend = pd.DataFrame({'image_id': train_join['image_id']})\n",
    "    for t in TARGETS:\n",
    "        a = train_join[f'{t}_py'].values\n",
    "        b = train_join[f'{t}_r'].values\n",
    "        b = np.where(np.isnan(b), a, b)  # replace NA with python preds\n",
    "        blend[t] = 0.5*a + 0.5*b\n",
    "\n",
    "    blend_long = preds_wide_to_long(train_join['image_id'], blend[TARGETS])\n",
    "    ens_scores = weighted_r2_from_long(true_long, blend_long)\n",
    "    print('Ensemble weighted R^2 (train OOF):', ens_scores)\n",
    "else:\n",
    "    print('Skipping R ensemble â€” rpy2 not available.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1417eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Final training on all data and test prediction\n",
    "# ===============================================================\n",
    "\n",
    "# Python final models already fit in python_models dict\n",
    "if HAVE_TEST_FEATURES:\n",
    "    # Prepare test feature matrix with same feature engineering\n",
    "    test_feat = test_meta.copy()\n",
    "    if 'Sampling_Date' in test_feat.columns:\n",
    "        test_feat['Sampling_Date'] = pd.to_datetime(test_feat['Sampling_Date'], errors='coerce')\n",
    "        test_feat['Year']  = test_feat['Sampling_Date'].dt.year\n",
    "        test_feat['Month'] = test_feat['Sampling_Date'].dt.month\n",
    "        test_feat = test_feat.drop(columns=['Sampling_Date'])\n",
    "    \n",
    "    # Predict with Python models\n",
    "    py_test_preds = pd.DataFrame({'image_id': test_feat['image_id'].values})\n",
    "    for t in TARGETS:\n",
    "        pred = python_models[t].predict(test_feat[[c for c in feat_cols if c != 'Sampling_Date']])\n",
    "        \n",
    "        # Transform back from log-space if needed\n",
    "        if USE_LOG_SPACE:\n",
    "            pred = np.expm1(pred)\n",
    "            pred = np.maximum(pred, 0)\n",
    "        \n",
    "        # Apply isotonic calibration if available\n",
    "        if USE_ISOTONIC_CALIBRATION and python_calibrators:\n",
    "            pred = python_calibrators[t].predict(pred)\n",
    "        \n",
    "        py_test_preds[t] = pred\n",
    "else:\n",
    "    # Fall back to per-target means\n",
    "    print('Warning: Test lacks features. Using per-target mean baseline.')\n",
    "    means = train.groupby('target_name')['target'].mean()\n",
    "    py_test_preds = pd.DataFrame({'image_id': sorted(test['image_id'].unique())})\n",
    "    for t in TARGETS:\n",
    "        py_test_preds[t] = float(means.get(t, train[train['target_name']==t]['target'].mean()))\n",
    "\n",
    "# If we have R test preds and they are valid, blend; else just Python\n",
    "final_preds_wide = py_test_preds.copy()\n",
    "if have_r:\n",
    "    # Align R test preds\n",
    "    r_te = r_test_preds.copy()\n",
    "    merged = final_preds_wide.merge(r_te, on='image_id', how='left', suffixes=('_py','_r'))\n",
    "    for t in TARGETS:\n",
    "        a = merged[f'{t}_py'].values\n",
    "        if f'{t}_r' in merged.columns:\n",
    "            b = merged[f'{t}_r'].values\n",
    "            if np.all(np.isnan(b)):\n",
    "                final_preds_wide[t] = a\n",
    "            else:\n",
    "                b = np.where(np.isnan(b), a, b)\n",
    "                final_preds_wide[t] = 0.5*a + 0.5*b\n",
    "        else:\n",
    "            final_preds_wide[t] = a\n",
    "    final_preds_wide = final_preds_wide[['image_id'] + TARGETS]\n",
    "\n",
    "# Apply physics constraints to final predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    print('Applying physics constraints to final predictions...')\n",
    "    final_preds_constrained = apply_physical_constraints(final_preds_wide[TARGETS])\n",
    "    final_preds_wide[TARGETS] = final_preds_constrained\n",
    "\n",
    "# Create submission\n",
    "sub_long = preds_wide_to_long(final_preds_wide['image_id'], final_preds_wide[TARGETS])\n",
    "submission = long_submission(sub_long)\n",
    "\n",
    "print('\\n=== Submission Preview ===')\n",
    "print(submission.head(10))\n",
    "print(f'\\nSubmission shape: {submission.shape}')\n",
    "print(f'Expected samples: {len(test[\"image_id\"].unique()) * 5}')\n",
    "print(f'\\nTarget statistics:')\n",
    "print(submission['target'].describe())\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('\\nâœ“ Wrote submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
