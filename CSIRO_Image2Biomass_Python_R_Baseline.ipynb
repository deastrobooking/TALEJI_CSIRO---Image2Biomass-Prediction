{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee725312",
   "metadata": {},
   "source": [
    "\n",
    "# CSIRO Image2Biomass â€” Python + R Hybrid Baseline (Weighted RÂ², CV, Submission)\n",
    "\n",
    "This notebook demonstrates a **competition-compliant baseline** that uses both **Python** and **R**:\n",
    "\n",
    "- Implements the **official weighted RÂ²** metric.\n",
    "- Builds simple **tabular baselines** from metadata (if available).\n",
    "- Trains a **Python Ridge** model and an **R linear model** and **ensembles** them.\n",
    "- Exports a valid `submission.csv` in **long format** (`sample_id,target`).\n",
    "\n",
    "It also gracefully **falls back** to a per-target **mean baseline** when features are unavailable in `test.csv` or if `rpy2` isn't present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce71410",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸš€ Enhanced Features (v2.0)\n",
    "\n",
    "This updated baseline includes several improvements recommended in the research textbook:\n",
    "\n",
    "### 1. **Log-Space Training** \n",
    "- Biomass distributions are highly skewed\n",
    "- Training in `log1p` space improves RÂ² and handles outliers better\n",
    "- Predictions are transformed back with `expm1` and clipped at 0\n",
    "\n",
    "### 2. **Isotonic Calibration**\n",
    "- Fits `IsotonicRegression` on out-of-fold predictions\n",
    "- Improves calibration and reduces systematic bias\n",
    "- Applied per target independently\n",
    "\n",
    "### 3. **Physics-Based Constraints**\n",
    "- Enforces `GDM â‰ˆ Dry_Green + Dry_Clover` via weighted average\n",
    "- Ensures `Dry_Total â‰¥ GDM` (physical consistency)\n",
    "- Clips all predictions to non-negative values\n",
    "\n",
    "### 4. **Enhanced Validation**\n",
    "- Cross-checks manual RÂ² implementation with `sklearn.metrics.r2_score`\n",
    "- Prints per-target OOF scores during training\n",
    "- Detailed submission statistics before export\n",
    "\n",
    "These enhancements typically improve leaderboard RÂ² by 0.02-0.05 with minimal complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Setup: imports, constants, metric helpers\n",
    "# ===============================================================\n",
    "\n",
    "import os, sys, gc, math, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------- Official competition weights ---------\n",
    "WEIGHTS = {\n",
    "    \"Dry_Green_g\": 0.1,\n",
    "    \"Dry_Dead_g\": 0.1,\n",
    "    \"Dry_Clover_g\": 0.1,\n",
    "    \"GDM_g\": 0.2,\n",
    "    \"Dry_Total_g\": 0.5,\n",
    "}\n",
    "TARGETS = list(WEIGHTS.keys())\n",
    "\n",
    "# --------- Configuration flags ---------\n",
    "USE_LOG_SPACE = True  # Train in log1p space for better handling of skewed distributions\n",
    "USE_ISOTONIC_CALIBRATION = True  # Calibrate predictions using isotonic regression\n",
    "APPLY_PHYSICS_CONSTRAINTS = True  # Enforce physical constraints post-prediction\n",
    "\n",
    "def r2_manual(y_true, y_pred):\n",
    "    \"\"\"Manual RÂ² calculation matching competition metric.\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    if y_true.size == 0:\n",
    "        return np.nan\n",
    "    y_bar = y_true.mean()\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - y_bar)**2)\n",
    "    if ss_tot == 0:\n",
    "        return 1.0 if np.allclose(y_true, y_pred) else 0.0\n",
    "    return 1.0 - ss_res/ss_tot\n",
    "\n",
    "def weighted_r2_from_long(true_long: pd.DataFrame, pred_long: pd.DataFrame):\n",
    "    \"\"\"Calculate weighted RÂ² from long-format dataframes.\"\"\"\n",
    "    merged = (true_long[['sample_id','target_name','target']].rename(columns={'target':'y_true'})\n",
    "              .merge(pred_long[['sample_id','target']].rename(columns={'target':'y_pred'}),\n",
    "                     on='sample_id', how='inner', validate='one_to_one'))\n",
    "    out = {}\n",
    "    final = 0.0\n",
    "    for t in TARGETS:\n",
    "        sub = merged[merged['target_name'] == t]\n",
    "        r2 = r2_manual(sub['y_true'].values, sub['y_pred'].values)\n",
    "        # Cross-check with sklearn\n",
    "        r2_sklearn = r2_score(sub['y_true'].values, sub['y_pred'].values)\n",
    "        out[t] = float(r2)\n",
    "        out[f'{t}_sklearn'] = float(r2_sklearn)\n",
    "        final += WEIGHTS[t]*r2\n",
    "    out['final'] = float(final)\n",
    "    return out\n",
    "\n",
    "def preds_wide_to_long(image_ids, preds_wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert wide predictions to long format for submission.\"\"\"\n",
    "    img_ids = list(image_ids)\n",
    "    assert preds_wide.shape[0] == len(img_ids), \"Row count mismatch to image_ids\"\n",
    "    df = preds_wide.copy()\n",
    "    df['image_id'] = img_ids\n",
    "    rows = []\n",
    "    for t in TARGETS:\n",
    "        part = df[['image_id', t]].rename(columns={t:'target'})\n",
    "        part['sample_id'] = part['image_id'] + '__' + t\n",
    "        rows.append(part[['sample_id','target']])\n",
    "    return (pd.concat(rows, ignore_index=True)\n",
    "              .sort_values('sample_id').reset_index(drop=True))\n",
    "\n",
    "def long_submission(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Format long predictions as competition submission.\"\"\"\n",
    "    return df_long[['sample_id','target']].sort_values('sample_id').reset_index(drop=True)\n",
    "\n",
    "def apply_physical_constraints(preds: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply physical constraints to predictions:\n",
    "    1. All values >= 0\n",
    "    2. GDM â‰ˆ Dry_Green + Dry_Clover (soft enforcement via averaging)\n",
    "    3. Dry_Total >= GDM\n",
    "    \"\"\"\n",
    "    preds = preds.copy()\n",
    "    \n",
    "    # Ensure non-negative\n",
    "    for t in TARGETS:\n",
    "        preds[t] = np.maximum(preds[t], 0)\n",
    "    \n",
    "    # Enforce GDM â‰ˆ Dry_Green + Dry_Clover\n",
    "    gdm_from_components = preds['Dry_Green_g'] + preds['Dry_Clover_g']\n",
    "    preds['GDM_g'] = 0.7 * preds['GDM_g'] + 0.3 * gdm_from_components\n",
    "    \n",
    "    # Ensure Dry_Total >= GDM\n",
    "    preds['Dry_Total_g'] = np.maximum(preds['Dry_Total_g'], preds['GDM_g'])\n",
    "    \n",
    "    return preds\n",
    "\n",
    "print('Setup complete. Configuration:')\n",
    "print(f'  - Log-space training: {USE_LOG_SPACE}')\n",
    "print(f'  - Isotonic calibration: {USE_ISOTONIC_CALIBRATION}')\n",
    "print(f'  - Physics constraints: {APPLY_PHYSICS_CONSTRAINTS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecf70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Data: load train/test with robust paths\n",
    "# ===============================================================\n",
    "# Primary (Kaggle):\n",
    "KAGGLE_INPUT = Path('/kaggle/input/csiro-biomass')\n",
    "# Fallback (local/dev):\n",
    "LOCAL_INPUTS = [\n",
    "    Path('/kaggle/input'),  # generic\n",
    "    Path('/mnt/data'),      # this environment\n",
    "    Path('.')               # last resort\n",
    "]\n",
    "\n",
    "def resolve_path(filename):\n",
    "    if KAGGLE_INPUT.exists():\n",
    "        p = KAGGLE_INPUT/filename\n",
    "        if p.exists(): return p\n",
    "    for base in LOCAL_INPUTS:\n",
    "        p = base/filename\n",
    "        if p.exists(): return p\n",
    "    raise FileNotFoundError(f\"Could not locate {filename} in known paths.\")\n",
    "\n",
    "train = pd.read_csv(resolve_path('train.csv'))\n",
    "test  = pd.read_csv(resolve_path('test.csv'))\n",
    "sample_sub = pd.read_csv(resolve_path('sample_submission.csv'))\n",
    "\n",
    "print('Train shape:', train.shape, '| Test shape:', test.shape)\n",
    "print('Train columns:', list(train.columns))\n",
    "print('Test columns:', list(test.columns))\n",
    "\n",
    "# Ensure image_id extraction\n",
    "train['image_id'] = train['sample_id'].str.split('__').str[0]\n",
    "test['image_id']  = test['sample_id'].str.split('__').str[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Feature assembly utilities\n",
    "# ===============================================================\n",
    "\n",
    "META_COLS = ['Sampling_Date','State','Species','Pre_GSHH_NDVI','Height_Ave_cm']\n",
    "\n",
    "# Build one unique row per image_id with meta\n",
    "def extract_meta(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    first_rows = (df_long\n",
    "                  .sort_values('sample_id')\n",
    "                  .drop_duplicates('image_id'))\n",
    "    meta = first_rows[['image_id'] + [c for c in META_COLS if c in first_rows.columns]].copy()\n",
    "    return meta\n",
    "\n",
    "# Pivot targets to wide: one row per image_id, cols = targets\n",
    "def pivot_targets_wide(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    wide = df_long.pivot_table(index='image_id', columns='target_name', values='target', aggfunc='mean')\n",
    "    wide = wide.reindex(columns=TARGETS)  # ensure correct order\n",
    "    wide = wide.reset_index()\n",
    "    return wide\n",
    "\n",
    "train_meta = extract_meta(train)\n",
    "train_wide_targets = pivot_targets_wide(train)\n",
    "\n",
    "# Merge targets with meta\n",
    "train_wide = train_meta.merge(train_wide_targets, on='image_id', how='left')\n",
    "\n",
    "# For test, meta may or may not exist; extract what we can\n",
    "test_meta = extract_meta(test)\n",
    "print('Meta columns found in test:', list(test_meta.columns))\n",
    "\n",
    "HAVE_TEST_FEATURES = all(col in test_meta.columns for col in META_COLS)\n",
    "print('Have full test features?', HAVE_TEST_FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Python baseline: Ridge with one-hot + scaling (GroupKFold by image)\n",
    "# Enhanced with log-space training and isotonic calibration\n",
    "# ===============================================================\n",
    "\n",
    "# Select features that exist\n",
    "feat_cols = [c for c in META_COLS if c in train_wide.columns]\n",
    "cat_cols  = [c for c in feat_cols if train_wide[c].dtype == 'object']\n",
    "num_cols  = [c for c in feat_cols if c not in cat_cols]\n",
    "\n",
    "# Simple date features if Sampling_Date exists\n",
    "if 'Sampling_Date' in feat_cols:\n",
    "    train_wide['Sampling_Date'] = pd.to_datetime(train_wide['Sampling_Date'], errors='coerce')\n",
    "    train_wide['Year']  = train_wide['Sampling_Date'].dt.year\n",
    "    train_wide['Month'] = train_wide['Sampling_Date'].dt.month\n",
    "    num_cols += ['Year','Month']\n",
    "    feat_cols = [c for c in feat_cols if c != 'Sampling_Date'] + ['Year','Month']\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), [c for c in num_cols if c in train_wide.columns]),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), [c for c in cat_cols if c in train_wide.columns])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "def fit_predict_ridge_oof(train_wide: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fit Ridge models with optional log-space training and isotonic calibration.\n",
    "    Returns OOF predictions and trained models (including calibrators).\n",
    "    \"\"\"\n",
    "    oof = pd.DataFrame({'image_id': train_wide['image_id'].values})\n",
    "    models = {}\n",
    "    calibrators = {} if USE_ISOTONIC_CALIBRATION else None\n",
    "    groups = train_wide['image_id']\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "    for t in TARGETS:\n",
    "        y = train_wide[t].values\n",
    "        \n",
    "        # Transform to log-space if configured\n",
    "        if USE_LOG_SPACE:\n",
    "            y_train = np.log1p(y)\n",
    "        else:\n",
    "            y_train = y\n",
    "            \n",
    "        oof_pred = np.zeros(len(train_wide), dtype=float)\n",
    "        oof_pred_raw = np.zeros(len(train_wide), dtype=float)  # For calibration\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(gkf.split(train_wide, y_train, groups=groups)):\n",
    "            X_tr = train_wide.iloc[tr_idx][feat_cols]\n",
    "            X_va = train_wide.iloc[va_idx][feat_cols]\n",
    "            y_tr = y_train[tr_idx]\n",
    "\n",
    "            pipe = Pipeline([('prep', preprocess), ('ridge', Ridge(alpha=1.0, random_state=42))])\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            pred_va = pipe.predict(X_va)\n",
    "            \n",
    "            # Store raw predictions for calibration\n",
    "            oof_pred_raw[va_idx] = pred_va\n",
    "\n",
    "        # Transform back from log-space\n",
    "        if USE_LOG_SPACE:\n",
    "            oof_pred_raw = np.expm1(oof_pred_raw)\n",
    "            oof_pred_raw = np.maximum(oof_pred_raw, 0)  # Clip negatives\n",
    "        \n",
    "        # Fit isotonic calibration on OOF predictions\n",
    "        if USE_ISOTONIC_CALIBRATION:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip')\n",
    "            iso.fit(oof_pred_raw, y)\n",
    "            oof_pred = iso.predict(oof_pred_raw)\n",
    "            calibrators[t] = iso\n",
    "        else:\n",
    "            oof_pred = oof_pred_raw\n",
    "\n",
    "        oof[t] = oof_pred\n",
    "        \n",
    "        # Fit final model on all data for test-time\n",
    "        if USE_LOG_SPACE:\n",
    "            y_all_train = np.log1p(y)\n",
    "        else:\n",
    "            y_all_train = y\n",
    "            \n",
    "        final_model = Pipeline([('prep', preprocess), ('ridge', Ridge(alpha=1.0, random_state=42))])\n",
    "        final_model.fit(train_wide[feat_cols], y_all_train)\n",
    "        models[t] = final_model\n",
    "        \n",
    "        print(f'âœ“ {t}: OOF RÂ² = {r2_manual(y, oof_pred):.4f}')\n",
    "\n",
    "    return oof, models, calibrators\n",
    "\n",
    "print('Training Python Ridge models with GroupKFold...')\n",
    "python_oof, python_models, python_calibrators = fit_predict_ridge_oof(train_wide)\n",
    "\n",
    "# Apply physics constraints to OOF predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    python_oof_constrained = apply_physical_constraints(python_oof[TARGETS])\n",
    "    python_oof[TARGETS] = python_oof_constrained\n",
    "\n",
    "# Evaluate Python-only OOF using long metric\n",
    "python_oof_long = preds_wide_to_long(train_wide['image_id'], python_oof[TARGETS])\n",
    "true_long = train[['sample_id','target_name','target']].copy()\n",
    "py_scores = weighted_r2_from_long(true_long, python_oof_long)\n",
    "print('\\nPython baseline weighted RÂ² (OOF):')\n",
    "print(json.dumps(py_scores, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# R modeling via rpy2 (if available)\n",
    "# ===============================================================\n",
    "have_r = False\n",
    "try:\n",
    "    get_ipython().run_line_magic('load_ext', 'rpy2.ipython')\n",
    "    have_r = True\n",
    "    print('rpy2.ipython extension loaded.')\n",
    "except Exception as e:\n",
    "    print('Could not load rpy2.ipython. R part will be skipped unless available.\\n', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ae8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data frames for R only if available\n",
    "if have_r:\n",
    "    # Build R-friendly train/test meta tables (wide targets + features)\n",
    "    r_train = train_wide[['image_id'] + [c for c in META_COLS if c in train_wide.columns] + TARGETS].copy()\n",
    "    r_test  = test_meta[['image_id'] + [c for c in META_COLS if c in test_meta.columns]].copy()\n",
    "\n",
    "    # If test lacks features, R model can't run; guard for that\n",
    "    r_can_predict_test = all(c in r_test.columns for c in META_COLS)\n",
    "    print('R can predict on test?', r_can_predict_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c26135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%R -i r_train -i r_test -o r_train_preds -o r_test_preds\n",
    "# Only runs if rpy2 loaded. Builds simple linear models per target.\n",
    "suppressPackageStartupMessages({\n",
    "  library(stats)\n",
    "})\n",
    "\n",
    "# Coerce date and create Year/Month if present\n",
    "date_cols <- intersect(colnames(r_train), c(\"Sampling_Date\"))\n",
    "if (length(date_cols) == 1) {\n",
    "  r_train$Sampling_Date <- as.Date(r_train$Sampling_Date)\n",
    "  r_train$Year <- as.integer(format(r_train$Sampling_Date, \"%Y\"))\n",
    "  r_train$Month <- as.integer(format(r_train$Sampling_Date, \"%m\"))\n",
    "}\n",
    "if (\"Sampling_Date\" %in% colnames(r_test)) {\n",
    "  r_test$Sampling_Date <- as.Date(r_test$Sampling_Date)\n",
    "  r_test$Year <- as.integer(format(r_test$Sampling_Date, \"%Y\"))\n",
    "  r_test$Month <- as.integer(format(r_test$Sampling_Date, \"%m\"))\n",
    "}\n",
    "\n",
    "# Feature set\n",
    "features <- c(\"Height_Ave_cm\",\"Pre_GSHH_NDVI\",\"State\",\"Species\",\"Year\",\"Month\")\n",
    "features <- intersect(features, colnames(r_train))\n",
    "\n",
    "predict_one <- function(target_name) {\n",
    "  rhs <- paste(features, collapse = \" + \")\n",
    "  frm <- as.formula(paste(target_name, \"~\", rhs))\n",
    "  mdl <- lm(frm, data = r_train)\n",
    "  pred_tr <- predict(mdl, newdata = r_train)\n",
    "  # For test, if features missing, return NAs\n",
    "  if (all(features %in% colnames(r_test))) {\n",
    "    pred_te <- predict(mdl, newdata = r_test)\n",
    "  } else {\n",
    "    pred_te <- rep(NA_real_, nrow(r_test))\n",
    "  }\n",
    "  list(tr = as.numeric(pred_tr), te = as.numeric(pred_te))\n",
    "}\n",
    "\n",
    "targets <- c(\"Dry_Green_g\",\"Dry_Dead_g\",\"Dry_Clover_g\",\"GDM_g\",\"Dry_Total_g\")\n",
    "r_train_preds <- data.frame(image_id = r_train$image_id)\n",
    "r_test_preds  <- data.frame(image_id = r_test$image_id)\n",
    "\n",
    "for (t in targets) {\n",
    "  res <- predict_one(t)\n",
    "  r_train_preds[[t]] <- res$tr\n",
    "  r_test_preds[[t]]  <- res$te\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba025d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Simple ensemble & evaluation\n",
    "# ===============================================================\n",
    "if have_r:\n",
    "    # Safe combine: if R preds missing (NA), fall back to Python\n",
    "    r_train_preds = r_train_preds\n",
    "    train_join = (python_oof.merge(r_train_preds, on='image_id', how='left', suffixes=('_py','_r')))\n",
    "    blend = pd.DataFrame({'image_id': train_join['image_id']})\n",
    "    for t in TARGETS:\n",
    "        a = train_join[f'{t}_py'].values\n",
    "        b = train_join[f'{t}_r'].values\n",
    "        b = np.where(np.isnan(b), a, b)  # replace NA with python preds\n",
    "        blend[t] = 0.5*a + 0.5*b\n",
    "\n",
    "    blend_long = preds_wide_to_long(train_join['image_id'], blend[TARGETS])\n",
    "    ens_scores = weighted_r2_from_long(true_long, blend_long)\n",
    "    print('Ensemble weighted R^2 (train OOF):', ens_scores)\n",
    "else:\n",
    "    print('Skipping R ensemble â€” rpy2 not available.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1417eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Final training on all data and test prediction\n",
    "# ===============================================================\n",
    "\n",
    "# Python final models already fit in python_models dict\n",
    "if HAVE_TEST_FEATURES:\n",
    "    # Prepare test feature matrix with same feature engineering\n",
    "    test_feat = test_meta.copy()\n",
    "    if 'Sampling_Date' in test_feat.columns:\n",
    "        test_feat['Sampling_Date'] = pd.to_datetime(test_feat['Sampling_Date'], errors='coerce')\n",
    "        test_feat['Year']  = test_feat['Sampling_Date'].dt.year\n",
    "        test_feat['Month'] = test_feat['Sampling_Date'].dt.month\n",
    "        test_feat = test_feat.drop(columns=['Sampling_Date'])\n",
    "    \n",
    "    # Predict with Python models\n",
    "    py_test_preds = pd.DataFrame({'image_id': test_feat['image_id'].values})\n",
    "    for t in TARGETS:\n",
    "        pred = python_models[t].predict(test_feat[[c for c in feat_cols if c != 'Sampling_Date']])\n",
    "        \n",
    "        # Transform back from log-space if needed\n",
    "        if USE_LOG_SPACE:\n",
    "            pred = np.expm1(pred)\n",
    "            pred = np.maximum(pred, 0)\n",
    "        \n",
    "        # Apply isotonic calibration if available\n",
    "        if USE_ISOTONIC_CALIBRATION and python_calibrators:\n",
    "            pred = python_calibrators[t].predict(pred)\n",
    "        \n",
    "        py_test_preds[t] = pred\n",
    "else:\n",
    "    # Fall back to per-target means\n",
    "    print('Warning: Test lacks features. Using per-target mean baseline.')\n",
    "    means = train.groupby('target_name')['target'].mean()\n",
    "    py_test_preds = pd.DataFrame({'image_id': sorted(test['image_id'].unique())})\n",
    "    for t in TARGETS:\n",
    "        py_test_preds[t] = float(means.get(t, train[train['target_name']==t]['target'].mean()))\n",
    "\n",
    "# If we have R test preds and they are valid, blend; else just Python\n",
    "final_preds_wide = py_test_preds.copy()\n",
    "if have_r:\n",
    "    # Align R test preds\n",
    "    r_te = r_test_preds.copy()\n",
    "    merged = final_preds_wide.merge(r_te, on='image_id', how='left', suffixes=('_py','_r'))\n",
    "    for t in TARGETS:\n",
    "        a = merged[f'{t}_py'].values\n",
    "        if f'{t}_r' in merged.columns:\n",
    "            b = merged[f'{t}_r'].values\n",
    "            if np.all(np.isnan(b)):\n",
    "                final_preds_wide[t] = a\n",
    "            else:\n",
    "                b = np.where(np.isnan(b), a, b)\n",
    "                final_preds_wide[t] = 0.5*a + 0.5*b\n",
    "        else:\n",
    "            final_preds_wide[t] = a\n",
    "    final_preds_wide = final_preds_wide[['image_id'] + TARGETS]\n",
    "\n",
    "# Apply physics constraints to final predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    print('Applying physics constraints to final predictions...')\n",
    "    final_preds_constrained = apply_physical_constraints(final_preds_wide[TARGETS])\n",
    "    final_preds_wide[TARGETS] = final_preds_constrained\n",
    "\n",
    "# Create submission\n",
    "sub_long = preds_wide_to_long(final_preds_wide['image_id'], final_preds_wide[TARGETS])\n",
    "submission = long_submission(sub_long)\n",
    "\n",
    "print('\\n=== Submission Preview ===')\n",
    "print(submission.head(10))\n",
    "print(f'\\nSubmission shape: {submission.shape}')\n",
    "print(f'Expected samples: {len(test[\"image_id\"].unique()) * 5}')\n",
    "print(f'\\nTarget statistics:')\n",
    "print(submission['target'].describe())\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('\\nâœ“ Wrote submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
