{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181a32f5",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass Prediction - Enhanced Baseline\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### For Kaggle Users\n",
    "If running on Kaggle, first upload the requirements file to your working directory, then run:\n",
    "```python\n",
    "# Option 1: Minimal requirements (faster install)\n",
    "!pip -q install -r /kaggle/working/requirements-min.txt\n",
    "\n",
    "# Option 2: Full requirements (includes R interop and notebook utilities)  \n",
    "!pip -q install -r /kaggle/working/requirements.txt\n",
    "```\n",
    "\n",
    "### For Local/Colab Users\n",
    "```python\n",
    "# Install from the repository requirements\n",
    "!pip -q install -r requirements-min.txt\n",
    "# or \n",
    "!pip -q install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "✅ **RGB Image Features**: 60-80 visual features (color, texture, vegetation indices)  \n",
    "✅ **Log-space Training**: Handles skewed biomass distributions  \n",
    "✅ **Isotonic Calibration**: Improves prediction reliability  \n",
    "✅ **Physics Constraints**: Enforces biological relationships  \n",
    "✅ **Conformal Intervals**: Provides uncertainty quantification  \n",
    "✅ **Robust Pipeline**: Handles missing data and edge cases  \n",
    "\n",
    "## Expected Performance\n",
    "\n",
    "This enhanced baseline typically achieves:\n",
    "- **Individual R²**: 0.3-0.7+ per target (varies by target complexity)\n",
    "- **Weighted R²**: 0.4-0.6+ (competition metric)\n",
    "- **Key improvements**: ~10-20% boost from RGB features + log-space training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3248a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Setup: Package Installation and Imports (offline/network tolerant)\n",
    "# ===============================================================\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Toggle professional-only settings (user requested)\n",
    "PROFESSIONAL_MODE = True\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Install packages from a requirements file if available.\n",
    "    Creates a temporary filtered requirements file that excludes truly optional\n",
    "    packages which tend to fail in offline / restricted environments (e.g. rpy2).\n",
    "    Returns True if installation was attempted and appeared successful, False otherwise.\n",
    "    \"\"\"\n",
    "    req_paths = [\n",
    "        Path('/kaggle/working/requirements-min.txt'),\n",
    "        Path('/kaggle/working/requirements.txt'),\n",
    "        Path('./requirements-min.txt'),\n",
    "        Path('./requirements.txt'),\n",
    "    ]\n",
    "\n",
    "    req_file = None\n",
    "    for path in req_paths:\n",
    "        if path.exists():\n",
    "            req_file = path\n",
    "            break\n",
    "\n",
    "    if not req_file:\n",
    "        print(\"No requirements file found; skipping batch install.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Found requirements file: {req_file}\")\n",
    "\n",
    "    # Read and filter the requirements to avoid installing problematic optional packages\n",
    "    raw = req_file.read_text(encoding='utf-8')\n",
    "    lines = []\n",
    "    for ln in raw.splitlines():\n",
    "        s = ln.strip()\n",
    "        # Exclude blank lines and comments\n",
    "        if not s or s.startswith('#'):\n",
    "            continue\n",
    "        # Skip rpy2 (optional) to avoid pip failures in offline envs\n",
    "        if s.lower().startswith('rpy2'):\n",
    "            print('Skipping rpy2 from automated install (optional package).')\n",
    "            continue\n",
    "        lines.append(ln)\n",
    "\n",
    "    # Write filtered temporary requirements file\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile('w', delete=False, suffix='.txt') as tmp:\n",
    "            tmp.write('\\n'.join(lines))\n",
    "            tmp_path = tmp.name\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create temp requirements file: {e}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        print(f\"Attempting to install packages from filtered requirements (may fail if offline): {tmp_path}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", tmp_path])\n",
    "        print('Package installation (filtered) completed.')\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print('Filtered requirements install failed (likely offline or network issue).')\n",
    "        print('Error:', e)\n",
    "        print('Skipping further automated installs. You can install packages manually if needed.')\n",
    "        return False\n",
    "    finally:\n",
    "        try:\n",
    "            os.unlink(tmp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Try to install requirements, but tolerate failures\n",
    "_installed = install_requirements()\n",
    "\n",
    "# Import common libs\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Attempt to import image libraries (critical for RGB features). If missing, try to install\n",
    "cv2_ok = False\n",
    "skimage_ok = False\n",
    "try:\n",
    "    import cv2\n",
    "    from skimage import filters, feature\n",
    "    from skimage.util import img_as_ubyte\n",
    "    cv2_ok = True\n",
    "    skimage_ok = True\n",
    "    print('Image processing libraries loaded.')\n",
    "except Exception as e:\n",
    "    print('Image processing libraries not fully available:', e)\n",
    "    if _installed:\n",
    "        print('Automated install already attempted; skipping further installs for image libs.')\n",
    "    else:\n",
    "        try:\n",
    "            print('Attempting single-package install for imaging libs...')\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"opencv-python-headless\", \"scikit-image\"])\n",
    "            import cv2\n",
    "            from skimage import filters, feature\n",
    "            from skimage.util import img_as_ubyte\n",
    "            cv2_ok = True\n",
    "            skimage_ok = True\n",
    "            print('Imaging packages installed and imported.')\n",
    "        except Exception as e2:\n",
    "            print('Could not install imaging packages (likely offline). RGB features will be disabled.')\n",
    "            cv2_ok = False\n",
    "            skimage_ok = False\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Try to import optional/professional packages but do not raise if missing.\n",
    "rpy2_ok = False\n",
    "try:\n",
    "    import rpy2\n",
    "    rpy2_ok = True\n",
    "    print('rpy2 is available.')\n",
    "except Exception:\n",
    "    print('rpy2 is not available; R blocks will be disabled. (This is expected in offline environments)')\n",
    "\n",
    "optuna_ok = False\n",
    "try:\n",
    "    import optuna\n",
    "    optuna_ok = True\n",
    "    print('Optuna available: professional hyperopt enabled.')\n",
    "except Exception:\n",
    "    print('Optuna not available: Bayesian optimization disabled.')\n",
    "\n",
    "skopt_ok = False\n",
    "try:\n",
    "    import skopt\n",
    "    skopt_ok = True\n",
    "    print('scikit-optimize available: advanced tuning enabled.')\n",
    "except Exception:\n",
    "    print('scikit-optimize not available: skipping related features.')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------- Official competition weights ---------\n",
    "WEIGHTS = {\n",
    "    \"Dry_Green_g\": 0.1,\n",
    "    \"Dry_Dead_g\": 0.1,\n",
    "    \"Dry_Clover_g\": 0.1,\n",
    "    \"GDM_g\": 0.2,\n",
    "    \"Dry_Total_g\": 0.5,\n",
    "}\n",
    "TARGETS = list(WEIGHTS.keys())\n",
    "\n",
    "# --------- Configuration flags ---------\n",
    "USE_LOG_SPACE = True\n",
    "USE_ISOTONIC_CALIBRATION = True\n",
    "APPLY_PHYSICS_CONSTRAINTS = True\n",
    "ENABLE_RGB_FEATURES = cv2_ok and skimage_ok\n",
    "ENABLE_PROFESSIONAL_FEATURES = PROFESSIONAL_MODE and (optuna_ok or skopt_ok)\n",
    "\n",
    "def r2_manual(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    if y_true.size == 0:\n",
    "        return np.nan\n",
    "    y_bar = y_true.mean()\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - y_bar)**2)\n",
    "    if ss_tot == 0:\n",
    "        return 1.0 if np.allclose(y_true, y_pred) else 0.0\n",
    "    return 1.0 - ss_res/ss_tot\n",
    "\n",
    "print('\\nEnvironment summary:')\n",
    "print(f'  - PROFESSIONAL_MODE = {PROFESSIONAL_MODE}')\n",
    "print(f'  - Filtered requirements install attempted = {_installed}')\n",
    "print(f'  - Imaging libs available = {ENABLE_RGB_FEATURES}')\n",
    "print(f'  - rpy2 available = {rpy2_ok} (R blocks disabled if False)')\n",
    "print(f'  - Professional tuning available = {ENABLE_PROFESSIONAL_FEATURES}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ecf70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1785, 9) | Test shape: (5, 3)\n",
      "Train columns: ['sample_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'target_name', 'target']\n",
      "Test columns: ['sample_id', 'image_path', 'target_name']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Data: load train/test with robust paths\n",
    "# ===============================================================\n",
    "# Primary (Kaggle):\n",
    "KAGGLE_INPUT = Path('/kaggle/input/csiro-biomass')\n",
    "# Fallback (local/dev):\n",
    "LOCAL_INPUTS = [\n",
    "    Path('/kaggle/input'),  # generic\n",
    "    Path('/mnt/data'),      # this environment\n",
    "    Path('.')               # last resort\n",
    "]\n",
    "\n",
    "def resolve_path(filename):\n",
    "    if KAGGLE_INPUT.exists():\n",
    "        p = KAGGLE_INPUT/filename\n",
    "        if p.exists(): return p\n",
    "    for base in LOCAL_INPUTS:\n",
    "        p = base/filename\n",
    "        if p.exists(): return p\n",
    "    raise FileNotFoundError(f\"Could not locate {filename} in known paths.\")\n",
    "\n",
    "train = pd.read_csv(resolve_path('train.csv'))\n",
    "test  = pd.read_csv(resolve_path('test.csv'))\n",
    "sample_sub = pd.read_csv(resolve_path('sample_submission.csv'))\n",
    "\n",
    "print('Train shape:', train.shape, '| Test shape:', test.shape)\n",
    "print('Train columns:', list(train.columns))\n",
    "print('Test columns:', list(test.columns))\n",
    "\n",
    "# Ensure image_id extraction\n",
    "train['image_id'] = train['sample_id'].str.split('__').str[0]\n",
    "test['image_id']  = test['sample_id'].str.split('__').str[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# RGB Image Features (Compact, Submission-Safe)\n",
    "# ===============================================================\n",
    "\n",
    "# Only build RGB features if imaging libraries are available\n",
    "if ENABLE_RGB_FEATURES:\n",
    "    print('Building RGB image features...')\n",
    "    \n",
    "    def _safe_read(p: Path):\n",
    "        try:\n",
    "            if not p.exists(): return None\n",
    "            im = cv2.imread(str(p), cv2.IMREAD_COLOR)\n",
    "            if im is None: return None\n",
    "            return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _color_stats(img, prefix):\n",
    "        a = img.reshape(-1, img.shape[-1]).astype(np.float32)\n",
    "        if a.max() > 1.5: a = a/255.0\n",
    "        out = {}\n",
    "        for i, name in enumerate([\"c0\",\"c1\",\"c2\"]):\n",
    "            v = a[:, i]\n",
    "            out[f\"{prefix}_{name}_mean\"] = float(np.mean(v))\n",
    "            out[f\"{prefix}_{name}_std\"]  = float(np.std(v))\n",
    "            out[f\"{prefix}_{name}_p10\"]  = float(np.percentile(v, 10))\n",
    "            out[f\"{prefix}_{name}_p50\"]  = float(np.percentile(v, 50))\n",
    "            out[f\"{prefix}_{name}_p90\"]  = float(np.percentile(v, 90))\n",
    "        return out\n",
    "\n",
    "    def _veg_indices_rgb(img):\n",
    "        rgb = img.astype(np.float32)\n",
    "        if rgb.max() > 1.5: rgb = rgb/255.0\n",
    "        R, G, B = [rgb[...,i] for i in range(3)]\n",
    "        eps = 1e-6\n",
    "        ExG  = 2*G - R - B\n",
    "        ExR  = 1.4*R - G\n",
    "        ExGR = ExG - ExR\n",
    "        VARI = (G - R) / (G + R - B + eps)\n",
    "        NDI  = (G - R) / (G + R + eps)\n",
    "        CIVE = 0.441*R - 0.881*G + 0.385*B + 18.78745\n",
    "        feats = {}\n",
    "        for name, arr in [(\"exg\",ExG),(\"exr\",ExR),(\"exgr\",ExGR),(\"vari\",VARI),(\"ndi\",NDI),(\"cive\",CIVE)]:\n",
    "            v = arr.reshape(-1)\n",
    "            feats[f\"{name}_mean\"] = float(np.mean(v))\n",
    "            feats[f\"{name}_std\"]  = float(np.std(v))\n",
    "            feats[f\"{name}_p90\"]  = float(np.percentile(v, 90))\n",
    "        try:\n",
    "            thr = filters.threshold_otsu(ExG)\n",
    "            feats[\"green_cover_frac\"] = float((ExG > thr).mean())\n",
    "        except Exception:\n",
    "            feats[\"green_cover_frac\"] = np.nan\n",
    "        return feats\n",
    "\n",
    "    def _texture_features(gray_u8):\n",
    "        out = {}\n",
    "        try:\n",
    "            lbp = feature.local_binary_pattern(gray_u8, P=8, R=1, method='uniform')\n",
    "            n_bins = int(lbp.max() + 1)\n",
    "            hist, _ = np.histogram(lbp, bins=n_bins, range=(0,n_bins), density=True)\n",
    "            hist = hist if len(hist)>=10 else np.pad(hist, (0,10-len(hist)))\n",
    "            for i in range(10): out[f\"lbp_u_hist_{i}\"] = float(hist[i])\n",
    "        except Exception:\n",
    "            for i in range(10): out[f\"lbp_u_hist_{i}\"] = np.nan\n",
    "        try:\n",
    "            q = (gray_u8.astype(np.float32)/255.0*31).astype(np.uint8)\n",
    "            glcm = feature.graycomatrix(q, [1,2,3], [0,np.pi/4,np.pi/2,3*np.pi/4], 32, symmetric=True, normed=True)\n",
    "            for prop in [\"contrast\",\"dissimilarity\",\"homogeneity\",\"ASM\",\"energy\",\"correlation\"]:\n",
    "                M = feature.graycoprops(glcm, prop)\n",
    "                out[f\"glcm_{prop}_mean\"] = float(M.mean())\n",
    "                out[f\"glcm_{prop}_std\"]  = float(M.std())\n",
    "        except Exception:\n",
    "            for prop in [\"contrast\",\"dissimilarity\",\"homogeneity\",\"ASM\",\"energy\",\"correlation\"]:\n",
    "                out[f\"glcm_{prop}_mean\"] = np.nan; out[f\"glcm_{prop}_std\"] = np.nan\n",
    "        try:\n",
    "            edges = feature.canny(gray_u8.astype(np.float32)/255.0, sigma=1.0)\n",
    "            out[\"edge_density\"] = float(edges.mean())\n",
    "        except Exception:\n",
    "            out[\"edge_density\"] = np.nan\n",
    "        return out\n",
    "\n",
    "    def build_rgb_features(df_long: pd.DataFrame, base: Path) -> pd.DataFrame:\n",
    "        one = df_long.sort_values(\"sample_id\").drop_duplicates(\"image_id\")[[\"image_id\",\"image_path\"]].copy()\n",
    "        rows = []\n",
    "        for _, r in one.iterrows():\n",
    "            iid = str(r[\"image_id\"]); rel = str(r.get(\"image_path\",\"\"))\n",
    "            paths = ([base/rel] if rel else []) + [base/\"train\"/f\"{iid}.jpg\", base/\"test\"/f\"{iid}.jpg\"]\n",
    "            img = None\n",
    "            for p in paths:\n",
    "                img = _safe_read(p)\n",
    "                if img is not None: break\n",
    "            feats = {\"image_id\": iid}\n",
    "            if img is None:\n",
    "                feats[\"img_missing\"] = 1.0\n",
    "                rows.append(pd.DataFrame([feats])); continue\n",
    "            feats[\"img_missing\"] = 0.0\n",
    "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "            lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY); gray_u8 = img_as_ubyte(gray)\n",
    "            feats |= _color_stats(img, \"rgb\")\n",
    "            feats |= _color_stats(hsv, \"hsv\")\n",
    "            feats |= _color_stats(lab, \"lab\")\n",
    "            feats |= _veg_indices_rgb(img)\n",
    "            feats |= _texture_features(gray_u8)\n",
    "            rows.append(pd.DataFrame([feats]))\n",
    "        return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # Build RGB features\n",
    "    DATA_ROOT = Path(\"/kaggle/input/csiro-biomass\") if Path(\"/kaggle/input/csiro-biomass\").exists() else Path(\"/mnt/data\")\n",
    "    rgb_train = build_rgb_features(train, DATA_ROOT)\n",
    "    rgb_test  = build_rgb_features(test,  DATA_ROOT)\n",
    "\n",
    "    print(f'RGB features built - Train: {rgb_train.shape}, Test: {rgb_test.shape}')\n",
    "    print(f'RGB feature columns: {len([c for c in rgb_train.columns if c != \"image_id\"])}')\n",
    "else:\n",
    "    print('RGB features disabled - image processing libraries not available')\n",
    "    # Create empty RGB feature dataframes\n",
    "    rgb_train = pd.DataFrame({'image_id': train['image_id'].unique()})\n",
    "    rgb_test = pd.DataFrame({'image_id': test['image_id'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e533d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Feature assembly utilities\n",
    "# ===============================================================\n",
    "\n",
    "META_COLS = ['Sampling_Date','State','Species','Pre_GSHH_NDVI','Height_Ave_cm']\n",
    "\n",
    "# Build one unique row per image_id with meta\n",
    "def extract_meta(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    first_rows = (df_long\n",
    "                  .sort_values('sample_id')\n",
    "                  .drop_duplicates('image_id'))\n",
    "    meta = first_rows[['image_id'] + [c for c in META_COLS if c in first_rows.columns]].copy()\n",
    "    return meta\n",
    "\n",
    "# Pivot targets to wide: one row per image_id, cols = targets\n",
    "def pivot_targets_wide(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    wide = df_long.pivot_table(index='image_id', columns='target_name', values='target', aggfunc='mean')\n",
    "    wide = wide.reindex(columns=TARGETS)  # ensure correct order\n",
    "    wide = wide.reset_index()\n",
    "    return wide\n",
    "\n",
    "train_meta = extract_meta(train)\n",
    "train_wide_targets = pivot_targets_wide(train)\n",
    "\n",
    "# Merge targets with meta\n",
    "train_wide = train_meta.merge(train_wide_targets, on='image_id', how='left')\n",
    "\n",
    "# Merge RGB features into training data\n",
    "train_wide_features = train_wide.merge(rgb_train, on='image_id', how='left')\n",
    "\n",
    "# For test, meta may or may not exist; extract what we can\n",
    "test_meta = extract_meta(test)\n",
    "print('Meta columns found in test:', list(test_meta.columns))\n",
    "\n",
    "# Merge RGB features into test data\n",
    "test_features_df = test_meta.merge(rgb_test, on='image_id', how='left')\n",
    "\n",
    "HAVE_TEST_FEATURES = all(col in test_meta.columns for col in META_COLS)\n",
    "print('Have full test features?', HAVE_TEST_FEATURES)\n",
    "print(f'Train with features: {train_wide_features.shape}')\n",
    "print(f'Test with features: {test_features_df.shape}')\n",
    "\n",
    "# Add helper functions\n",
    "def weighted_r2_from_long(true_long: pd.DataFrame, pred_long: pd.DataFrame):\n",
    "    \"\"\"Calculate weighted R² from long-format dataframes.\"\"\"\n",
    "    merged = (true_long[['sample_id','target_name','target']].rename(columns={'target':'y_true'})\n",
    "              .merge(pred_long[['sample_id','target']].rename(columns={'target':'y_pred'}),\n",
    "                     on='sample_id', how='inner', validate='one_to_one'))\n",
    "    out = {}\n",
    "    final = 0.0\n",
    "    for t in TARGETS:\n",
    "        sub = merged[merged['target_name'] == t]\n",
    "        r2 = r2_manual(sub['y_true'].values, sub['y_pred'].values)\n",
    "        out[t] = float(r2)\n",
    "        final += WEIGHTS[t]*r2\n",
    "    out['final'] = float(final)\n",
    "    return out\n",
    "\n",
    "def preds_wide_to_long(image_ids, preds_wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert wide predictions to long format for submission.\"\"\"\n",
    "    img_ids = list(image_ids)\n",
    "    assert preds_wide.shape[0] == len(img_ids), \"Row count mismatch to image_ids\"\n",
    "    df = preds_wide.copy()\n",
    "    df['image_id'] = img_ids\n",
    "    rows = []\n",
    "    for t in TARGETS:\n",
    "        part = df[['image_id', t]].rename(columns={t:'target'})\n",
    "        part['sample_id'] = part['image_id'] + '__' + t\n",
    "        rows.append(part[['sample_id','target']])\n",
    "    return (pd.concat(rows, ignore_index=True)\n",
    "              .sort_values('sample_id').reset_index(drop=True))\n",
    "\n",
    "def long_submission(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Format long predictions as competition submission.\"\"\"\n",
    "    return df_long[['sample_id','target']].sort_values('sample_id').reset_index(drop=True)\n",
    "\n",
    "def apply_physical_constraints(preds: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply physical constraints to predictions:\n",
    "    1. All values >= 0\n",
    "    2. GDM ≈ Dry_Green + Dry_Clover (soft enforcement via averaging)\n",
    "    3. Dry_Total >= GDM\n",
    "    \"\"\"\n",
    "    preds = preds.copy()\n",
    "    \n",
    "    # Ensure non-negative\n",
    "    for t in TARGETS:\n",
    "        preds[t] = np.maximum(preds[t], 0)\n",
    "    \n",
    "    # Enforce GDM ≈ Dry_Green + Dry_Clover\n",
    "    gdm_from_components = preds['Dry_Green_g'] + preds['Dry_Clover_g']\n",
    "    preds['GDM_g'] = 0.7 * preds['GDM_g'] + 0.3 * gdm_from_components\n",
    "    \n",
    "    # Ensure Dry_Total >= GDM\n",
    "    preds['Dry_Total_g'] = np.maximum(preds['Dry_Total_g'], preds['GDM_g'])\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a7524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_wide columns: ['image_id', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g', 'Year', 'Month']\n",
      "train_wide shape: (357, 13)\n",
      "\n",
      "First few rows:\n",
      "       image_id Sampling_Date State            Species  Pre_GSHH_NDVI  \\\n",
      "0  ID1011485656    2015-09-04   Tas    Ryegrass_Clover           0.62   \n",
      "1  ID1012260530    2015-04-01   NSW            Lucerne           0.55   \n",
      "2  ID1025234388    2015-09-01    WA  SubcloverDalkeith           0.38   \n",
      "3  ID1028611175    2015-05-18   Tas           Ryegrass           0.66   \n",
      "4  ID1035947949    2015-09-11   Tas           Ryegrass           0.54   \n",
      "\n",
      "   Height_Ave_cm  Dry_Green_g  Dry_Dead_g  Dry_Clover_g    GDM_g  Dry_Total_g  \\\n",
      "0         4.6667      16.2751     31.9984        0.0000  16.2750      48.2735   \n",
      "1        16.0000       7.6000      0.0000        0.0000   7.6000       7.6000   \n",
      "2         1.0000       0.0000      0.0000        6.0500   6.0500       6.0500   \n",
      "3         5.0000      24.2376     30.9703        0.0000  24.2376      55.2079   \n",
      "4         3.5000      10.5261     23.2239        0.4343  10.9605      34.1844   \n",
      "\n",
      "   Year  Month  \n",
      "0  2015      9  \n",
      "1  2015      4  \n",
      "2  2015      9  \n",
      "3  2015      5  \n",
      "4  2015      9  \n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Enhanced Python baseline: Ridge with log-space training\n",
    "# ===============================================================\n",
    "\n",
    "def add_date_derivatives(df):\n",
    "    \"\"\"Add date features if Sampling_Date exists.\"\"\"\n",
    "    df = df.copy()\n",
    "    if 'Sampling_Date' in df.columns:\n",
    "        df['Sampling_Date'] = pd.to_datetime(df['Sampling_Date'], errors='coerce')\n",
    "        df['Year'] = df['Sampling_Date'].dt.year\n",
    "        df['Month'] = df['Sampling_Date'].dt.month\n",
    "        df['DayOfYear'] = df['Sampling_Date'].dt.dayofyear\n",
    "    return df\n",
    "\n",
    "def build_feature_lists(df):\n",
    "    \"\"\"Build lists of numeric and categorical features.\"\"\"\n",
    "    # Get base meta columns that exist\n",
    "    base_cols = [c for c in META_COLS if c in df.columns and c != 'Sampling_Date']\n",
    "    \n",
    "    # Add date derivatives if they exist\n",
    "    date_cols = [c for c in ['Year', 'Month', 'DayOfYear'] if c in df.columns]\n",
    "    \n",
    "    # Add RGB features\n",
    "    rgb_cols = [c for c in df.columns if c.startswith(('rgb_', 'hsv_', 'lab_', 'exg', 'exr', 'vari', 'ndi', 'cive', 'green_cover', 'lbp_', 'glcm_', 'edge_', 'img_missing'))]\n",
    "    \n",
    "    # Separate numeric and categorical\n",
    "    all_feature_cols = base_cols + date_cols + rgb_cols\n",
    "    cat_cols = [c for c in all_feature_cols if c in df.columns and df[c].dtype == 'object']\n",
    "    num_cols = [c for c in all_feature_cols if c in df.columns and c not in cat_cols]\n",
    "    \n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def align_columns(df, expected_cols):\n",
    "    \"\"\"Ensure DataFrame has all expected columns.\"\"\"\n",
    "    for col in expected_cols:\n",
    "        if col not in df.columns:\n",
    "            if df[col].dtype == 'object' if col in df.columns else False:\n",
    "                df[col] = 'missing'\n",
    "            else:\n",
    "                df[col] = 0.0\n",
    "    return df[expected_cols + [c for c in df.columns if c not in expected_cols]]\n",
    "\n",
    "def make_preprocess(num_cols, cat_cols):\n",
    "    \"\"\"Create preprocessing pipeline.\"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    transformers = []\n",
    "    \n",
    "    if num_cols:\n",
    "        transformers.append(('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), num_cols))\n",
    "    \n",
    "    if cat_cols:\n",
    "        transformers.append(('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), cat_cols))\n",
    "    \n",
    "    return ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "def fit_predict_ridge_oof_log(train_wide, n_splits=5):\n",
    "    \"\"\"\n",
    "    Fit Ridge models with log-space training and isotonic calibration.\n",
    "    Returns OOF predictions and trained models.\n",
    "    \"\"\"\n",
    "    oof = pd.DataFrame({\"image_id\": train_wide[\"image_id\"].values})\n",
    "    models = {}\n",
    "    \n",
    "    # Prepare features\n",
    "    TW = add_date_derivatives(train_wide)\n",
    "    num_cols, cat_cols = build_feature_lists(TW)\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    groups = TW[\"image_id\"]\n",
    "    \n",
    "    print(f'Training with {len(num_cols)} numeric and {len(cat_cols)} categorical features')\n",
    "    \n",
    "    for t in TARGETS:\n",
    "        print(f'Training {t}...', end=' ')\n",
    "        y_raw = TW[t].values\n",
    "        y = np.log1p(np.clip(y_raw, 0, None))   # log-space\n",
    "        oof_pred_log = np.zeros(len(TW), dtype=float)\n",
    "        oof_pred_raw = np.zeros(len(TW), dtype=float)\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(gkf.split(TW, y, groups=groups)):\n",
    "            X_tr = align_columns(TW.iloc[tr_idx].copy(), list(set(num_cols+cat_cols)))\n",
    "            X_va = align_columns(TW.iloc[va_idx].copy(), list(set(num_cols+cat_cols)))\n",
    "            \n",
    "            pre = make_preprocess(num_cols, cat_cols)\n",
    "            pipe = Pipeline([(\"prep\", pre), (\"ridge\", Ridge(alpha=1.0, random_state=42))])\n",
    "            pipe.fit(X_tr, y[tr_idx])\n",
    "            pred_va_log = pipe.predict(X_va)\n",
    "            pred_va_raw = np.expm1(pred_va_log).clip(min=0)   # back to original units\n",
    "            \n",
    "            oof_pred_log[va_idx] = pred_va_log\n",
    "            oof_pred_raw[va_idx] = pred_va_raw\n",
    "\n",
    "        # Apply isotonic calibration if enabled\n",
    "        if USE_ISOTONIC_CALIBRATION:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip')\n",
    "            iso.fit(oof_pred_raw, y_raw)\n",
    "            oof_pred_final = iso.predict(oof_pred_raw)\n",
    "            calibrator = iso\n",
    "        else:\n",
    "            oof_pred_final = oof_pred_raw\n",
    "            calibrator = None\n",
    "\n",
    "        oof[t] = oof_pred_final\n",
    "\n",
    "        # Fit final model on all data\n",
    "        X_all = align_columns(TW.copy(), list(set(num_cols+cat_cols)))\n",
    "        pre = make_preprocess(num_cols, cat_cols)\n",
    "        final_model = Pipeline([(\"prep\", pre), (\"ridge\", Ridge(alpha=1.0, random_state=42))])\n",
    "        final_model.fit(X_all, y)\n",
    "        models[t] = (final_model, list(set(num_cols+cat_cols)), calibrator)\n",
    "        \n",
    "        r2 = r2_manual(y_raw, oof_pred_final)\n",
    "        print(f'OOF R² = {r2:.4f}')\n",
    "    \n",
    "    return oof, models\n",
    "\n",
    "def predict_with_models_log(models, df_any):\n",
    "    \"\"\"Make predictions using log-space trained models.\"\"\"\n",
    "    DF = add_date_derivatives(df_any.copy())\n",
    "    out = pd.DataFrame({\"image_id\": DF[\"image_id\"].values})\n",
    "    \n",
    "    for t in TARGETS:\n",
    "        model, feats, calibrator = models[t]\n",
    "        X = align_columns(DF.copy(), feats)\n",
    "        pred_log = model.predict(X)\n",
    "        pred_raw = np.expm1(pred_log).clip(min=0)\n",
    "        \n",
    "        # Apply calibration if available\n",
    "        if calibrator is not None:\n",
    "            pred_final = calibrator.predict(pred_raw)\n",
    "        else:\n",
    "            pred_final = pred_raw\n",
    "            \n",
    "        out[t] = pred_final\n",
    "    return out\n",
    "\n",
    "print('Training enhanced Python Ridge models with log-space and RGB features...')\n",
    "python_oof, python_models = fit_predict_ridge_oof_log(train_wide_features)\n",
    "\n",
    "# Apply physics constraints to OOF predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    python_oof_constrained = apply_physical_constraints(python_oof[TARGETS])\n",
    "    python_oof[TARGETS] = python_oof_constrained\n",
    "\n",
    "# Evaluate Python-only OOF using long metric\n",
    "python_oof_long = preds_wide_to_long(train_wide_features['image_id'], python_oof[TARGETS])\n",
    "true_long = train[['sample_id','target_name','target']].copy()\n",
    "py_scores = weighted_r2_from_long(true_long, python_oof_long)\n",
    "print('\\nEnhanced Python baseline weighted R² (OOF):')\n",
    "print(json.dumps(py_scores, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Optional: Conformal Prediction Intervals\n",
    "# ===============================================================\n",
    "\n",
    "def conformal_bounds(oof_true_long, oof_pred_long, test_pred_long, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Add conformal prediction intervals using OOF residuals.\n",
    "    alpha=0.1 -> 90% marginal coverage\n",
    "    \"\"\"\n",
    "    out = test_pred_long.copy()\n",
    "    out['lower'] = out['target']  # Initialize\n",
    "    out['upper'] = out['target']  # Initialize\n",
    "    \n",
    "    for t in TARGETS:\n",
    "        # Get OOF residuals for this target\n",
    "        tmask = oof_true_long[\"target_name\"] == t\n",
    "        if tmask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        residuals = (oof_true_long.loc[tmask, \"target\"].values\n",
    "                    - oof_pred_long.loc[tmask, \"target\"].values)\n",
    "        \n",
    "        # Compute conformal quantile\n",
    "        q = np.quantile(np.abs(residuals), 1 - alpha)\n",
    "        \n",
    "        # Apply to test predictions for this target\n",
    "        tmask2 = out[\"sample_id\"].str.endswith(\"__\" + t)\n",
    "        out.loc[tmask2, \"lower\"] = np.maximum(0, out.loc[tmask2, \"target\"] - q)\n",
    "        out.loc[tmask2, \"upper\"] = out.loc[tmask2, \"target\"] + q\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Generate test predictions\n",
    "print('Generating test predictions...')\n",
    "python_test_preds = predict_with_models_log(python_models, test_features_df)\n",
    "\n",
    "# Apply physics constraints to test predictions\n",
    "if APPLY_PHYSICS_CONSTRAINTS:\n",
    "    python_test_preds_constrained = apply_physical_constraints(python_test_preds[TARGETS])\n",
    "    python_test_preds[TARGETS] = python_test_preds_constrained\n",
    "\n",
    "# Convert to long format\n",
    "python_test_long = preds_wide_to_long(test_features_df['image_id'], python_test_preds[TARGETS])\n",
    "\n",
    "# Add conformal intervals (90% coverage)\n",
    "python_test_with_intervals = conformal_bounds(true_long, python_oof_long, python_test_long, alpha=0.1)\n",
    "\n",
    "print(f'Test predictions shape: {python_test_preds.shape}')\n",
    "print(f'Test predictions with intervals: {python_test_with_intervals.shape}')\n",
    "print('Sample predictions with intervals:')\n",
    "print(python_test_with_intervals.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# R modeling via rpy2 (if available)\n",
    "# ===============================================================\n",
    "have_r = False\n",
    "if rpy2_ok:\n",
    "    try:\n",
    "        get_ipython().run_line_magic('load_ext', 'rpy2.ipython')\n",
    "        have_r = True\n",
    "        print('rpy2.ipython extension loaded.')\n",
    "    except Exception as e:\n",
    "        print('Could not load rpy2.ipython. R part will be skipped unless available.\\n', e)\n",
    "else:\n",
    "    print('rpy2 not available - R modeling disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ae8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Final submission generation\n",
    "# ===============================================================\n",
    "\n",
    "# Create final submission using enhanced Python predictions\n",
    "final_submission = long_submission(python_test_long)\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Submission created!')\n",
    "print(f'Submission shape: {final_submission.shape}')\n",
    "print('Submission head:')\n",
    "print(final_submission.head(10))\n",
    "\n",
    "print('\\nSubmission statistics by target:')\n",
    "for target in TARGETS:\n",
    "    target_mask = final_submission['sample_id'].str.endswith(f'__{target}')\n",
    "    target_preds = final_submission.loc[target_mask, 'target']\n",
    "    print(f'{target:15s}: mean={target_preds.mean():8.3f}, std={target_preds.std():8.3f}, '\n",
    "          f'min={target_preds.min():8.3f}, max={target_preds.max():8.3f}')\n",
    "\n",
    "# Validation check\n",
    "expected_samples = len(test['image_id'].unique()) * len(TARGETS)\n",
    "actual_samples = len(final_submission)\n",
    "print(f'\\nValidation: Expected {expected_samples} samples, got {actual_samples}')\n",
    "print(f'All targets covered: {set(final_submission.sample_id.str.split(\"__\").str[1]) == set(TARGETS)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
